import collections
import copy

import torch
from torch import nn
from torch.nn import functional as F
import torch.utils.checkpoint as cp
import numpy as np
import math

from opencood.models.sub_modules.box_attention import Box3dAttention
from opencood.models.sub_modules.torch_transformation_utils import warp_affine_simple
from opencood.utils import box_utils

class MLP(nn.Module):
    """Very simple multi-layer perceptron (also called FFN)"""

    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super().__init__()
        self.num_layers = num_layers # 3
        h = [hidden_dim] * (num_layers - 1) # [256, 256]
        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)
        return x

class Transformer(nn.Module):
    def __init__(
        self,
        d_model=256,
        nhead=8,
        nlevel=1,
        num_encoder_layers=6,
        num_decoder_layers=6,
        dim_feedforward=1024,
        dropout=0.1,
        activation="relu",
        num_queries=300,
        num_classes=1,
        mom=0.999,
        cp_flag=False,
    ):
        super().__init__()

        self.num_queries = num_queries
        self.num_classes = num_classes
        self.m = mom
        self.extra_query_num = 200 # È¢ùÂ§ñÁöÑqueryÊï∞ÈáèÔºåÁî®‰∫éÈùûÈáçÂè†‰ΩçÁΩÆÁöÑË°•ÂÖÖ

        encoder_layer = TransformerEncoderLayer(d_model, nhead, nlevel, dim_feedforward, dropout, activation)
        self.encoder = TransformerEncoder(d_model, encoder_layer, num_encoder_layers)
        self.trans_adapter = TransAdapt(d_model, nhead, nlevel, dim_feedforward, dropout, activation)
        self.query_fusion = AttenQueryFusion(d_model)
        self.ref_fusion = AttenQueryFusion(7)

        decoder_layer = TransformerDecoderLayer(d_model, nhead, nlevel, dim_feedforward, dropout, activation)
        self.decoder = TransformerDecoder(d_model, decoder_layer, num_decoder_layers, cp_flag)

    def _create_ref_windows(self, tensor_list):
        device = tensor_list[0].device

        ref_windows = []
        for tensor in tensor_list:
            B, _, H, W = tensor.shape
            ref_y, ref_x = torch.meshgrid(
                torch.linspace(0.5, H - 0.5, H, dtype=torch.float32, device=device),
                torch.linspace(0.5, W - 0.5, W, dtype=torch.float32, device=device),
                indexing="ij",
            )

            ref_y = ref_y.reshape(-1)[None] / H
            ref_x = ref_x.reshape(-1)[None] / W
            ref_xy = torch.stack((ref_x, ref_y), -1)
            ref_wh = torch.ones_like(ref_xy) * 0.025  # 0.01 - 0.05 w.r.t. Deform-DETR
            placeholder = torch.zeros_like(ref_xy)[..., :1]
            ref_box = torch.cat((ref_xy, placeholder + 0.5, ref_wh, placeholder + 0.5, placeholder), -1).expand(
                B, -1, -1
            )

            ref_windows.append(ref_box)
        ref_windows = torch.cat(ref_windows, dim=1)

        return ref_windows

    def _get_enc_proposals(self, enc_embed, ref_windows, indexes=None):
        B, L = enc_embed.shape[:2]
        out_logits, out_ref_windows = self.proposal_head(enc_embed, ref_windows)

        out_probs = out_logits[..., 0].sigmoid()
        topk_probs, indexes = torch.topk(out_probs, self.num_queries + self.extra_query_num, dim=1, sorted=True)
        topk_probs = topk_probs.unsqueeze(-1)
        indexes = indexes.unsqueeze(-1)

        out_ref_windows = torch.gather(out_ref_windows, 1, indexes.expand(-1, -1, out_ref_windows.shape[-1]))
        out_ref_windows = torch.cat(
            (
                out_ref_windows.detach(),
                topk_probs.detach().expand(-1, -1, out_logits.shape[-1]),
            ),
            dim=-1,
        )

        out_pos = None
        out_embed = None

        return out_embed, out_pos, out_ref_windows, indexes

    @torch.no_grad()
    def _momentum_update_gt_decoder(self):
        """
        Momentum update of the key encoder
        """
        for param_q, param_k in zip(self.decoder.parameters(), self.decoder_gt.parameters()):
            param_k.data = param_k.data * self.m + param_q.data * (1.0 - self.m)

    def regroup(self, x, record_len):
        cum_sum_len = torch.cumsum(record_len, dim=0)
        split_x = torch.tensor_split(x, cum_sum_len[:-1].cpu())
        return split_x

    def forward(self, src, pos, noised_gt_box=None, noised_gt_onehot=None, attn_mask=None, targets=None, record_len=None, pairwise_t_matrix=None, pairwise_t_matrix_ref=None, box_encode_func=None, box_decode_func=None):
        '''
        src: [(B_n, 256, H, W)]
        pos: [(B_n, 256, H, W)]
        noised_gt_box: (B, pad_size, 7)  ËøôÈáåÁî®ÁöÑÂ∫îËØ•ÊòØÂçèÂêågt
        noised_gt_onehot: (B, pad_size, num_classes)
        attn_mask: (1000+pad_size, 1000+pad_size)
        targets: [{'gt_boxes': (N, 7), 'labels': (N, )}, ...]
        '''
        assert pos is not None, "position encoding is required!"
        src_anchors = self._create_ref_windows(src) # ÂàõÈÄ†ÂèÇËÄÉÊ°ÜÔºåËøô‰∏™ÊòØBoxAttentionÂøÖÈ°ªÁöÑ (B_n, HW, 7)
        src, _, src_shape = flatten_with_shape(src, None)# Â±ïÂπ≥ÁâπÂæÅÂõæÔºåËøîÂõûÁöÑÊòØ (B_n, H*W, 256), None, (1, 2) ÊúÄÂêé‰∏ÄÈ°πËÆ∞ÂΩïÁùÄHÔºåW Âç≥feature shape
        src_pos = []
        for pe in pos:
            B, C = pe.shape[:2]
            pe = pe.view(B, C, -1).transpose(1, 2) # b, h*w, c
            src_pos.append(pe)
        src_pos = torch.cat(src_pos, dim=1) # (B_n, H*W, C)
        src_start_index = torch.cat([src_shape.new_zeros(1), src_shape.prod(1).cumsum(0)[:-1]]) # ËøôÊòØ‰∏∫‰∫ÜÁîüÊàêÂàíÂàÜÁöÑÁ¥¢ÂºïÔºåÂå∫ÂàÜÊØè‰∏™ÁâπÂæÅÂõæÁöÑ‰ΩçÁΩÆÔºåÁî±‰∫éÂè™Êúâ‰∏Ä‰∏™ÁâπÂæÅÂõæÔºåÊâÄ‰ª•ÁªìÊûúÊòØ(0,)

        memory = self.encoder(src, src_pos, src_shape, src_start_index, src_anchors) # BoxAttention ÊèêÂèñÁâπÂæÅ ÁªìÊûú‰∏∫(B_n, H*W, 256)
        query_embed, query_pos, topk_proposals, topk_indexes = self._get_enc_proposals(memory, src_anchors) # ËøîÂõûNoneÔºåNoneÔºå(B_n, query_num+extra_num, 8)Ôºå(B_n, query_num+extra_num, 1)
        ego_topk_proposals = topk_proposals[:, :self.num_queries, :] # (B_n, query_num, 8)
        ego_topk_indexes = topk_indexes[:, :self.num_queries, :] # (B_n, query_num, 1)
        extra_topk_proposals = topk_proposals[:, self.num_queries:, :]  # (B_n, extra_num, 8)
        extra_topk_indexes = topk_indexes[:, self.num_queries:, :]  # (B_n, extra_num, 1)

        fined_query = torch.gather(memory, 1, ego_topk_indexes.expand(-1, -1, memory.shape[-1])) # (B_n, query_num, C) refineÁöÑquery
        extra_query = torch.gather(memory, 1, extra_topk_indexes.expand(-1, -1, memory.shape[-1])) # (B_n, extra_num, C) refineÁöÑquery

        H, W = src_shape[0,0], src_shape[0,1]
        memory_discrete = torch.zeros_like(memory) # (B_n, H*W, 256) 
        memory_discrete = memory_discrete.scatter(1, ego_topk_indexes.repeat(1, 1, memory_discrete.size(-1)), fined_query) # (B_n, H*W, 256) Â∞ÜqueryÊîæÂÖ•Âà∞‰∏Ä‰∏™Á©∫ÁöÑmemory‰∏≠
        memory_discrete = memory_discrete.permute(0, 2, 1).reshape(memory.shape[0], memory.shape[-1], H, W) # (B_n, C, H, W) ÂΩ¢ÊàêÁ®ÄÁñèÁöÑÁâπÂæÅÂõæ

        # Êñ∞Âª∫‰∏Ä‰∏™ÈªòËÆ§ÂèÇËÄÉÊ°ÜÔºåÁÑ∂ÂêéÂ∞ÜencoderÈ¢ÑÊµãÁöÑÂÜÖÂÆπÂ°´ÂÖÖËøõÂéªÔºåËøô‰∏™Â∞Ü‰ºöÂú®Á©∫Èó¥ÂèòÊç¢Âêé‰Ωú‰∏∫
        ref_boxes_before_trans = copy.deepcopy(src_anchors)
        ref_probs_before_trans = torch.zeros(ref_boxes_before_trans.size(0), ref_boxes_before_trans.size(1), 1).to(ref_boxes_before_trans)
        ref_boxes_before_trans = torch.cat([ref_boxes_before_trans, ref_probs_before_trans], dim=-1)
        fined_ref_boxes = ego_topk_proposals # (B_n, query_num, 8) Ëøô‰∏™ÊòØÂèÇËÄÉÊ°Ü Ë¶ÅË∑üÁùÄÈááÊ†∑
        ref_boxes_before_trans = ref_boxes_before_trans.scatter(1, ego_topk_indexes.repeat(1, 1, ref_boxes_before_trans.size(-1)), fined_ref_boxes) # (B_n, H*W, 8) Â∞ÜqueryÊîæÂÖ•Âà∞‰∏Ä‰∏™Á©∫ÁöÑmemory‰∏≠

        ref_boxes_before_trans = ref_boxes_before_trans.permute(0, 2, 1).reshape(memory.shape[0], 8, H, W) # (B_n, 8, H, W) ÂΩ¢ÊàêÁ®ÄÁñèÁöÑÁâπÂæÅÂõæ

        # ÂàõÈÄ†maskÊ†áËÆ∞fined query
        valid_flag = torch.ones(fined_query.shape[0], fined_query.shape[1], 1).to(fined_query) # (B_n, query_num, 1) ÂÖ®1
        memory_mask = torch.zeros(memory.shape[0], memory.shape[1], 1).to(memory) # (B_n, HW, 1)
        memory_mask = memory_mask.scatter(1, ego_topk_indexes.repeat(1, 1, memory_mask.size(-1)), valid_flag) # (B_n, HW, 1)  Â∞Üfined queryÁªôÊ†áËÆ∞
        memory_mask = memory_mask.permute(0, 2, 1).reshape(memory_mask.shape[0], 1, H, W) # (B_n, 1, H, W)

        memory_batch_lst = self.regroup(memory, record_len)
        memory_discrete_batch_lst = self.regroup(memory_discrete, record_len)
        ref_boxes_before_trans_batch_lst = self.regroup(ref_boxes_before_trans, record_len)
        memory_mask_batch_lst = self.regroup(memory_mask, record_len)

        ego_topk_indexes_batch_lst = self.regroup(ego_topk_indexes, record_len)
        extra_topk_indexes_batch_lst = self.regroup(extra_topk_indexes, record_len)
        extra_query_batch_lst = self.regroup(extra_query, record_len)
        extra_topk_proposals_batch_lst = self.regroup(extra_topk_proposals, record_len) #  [(N1, extra_num, 8), (N2, extra_num, 8)...]

        fused_queries = []
        fused_ref_windows = []
        fused_indicies = []
        ego_features = []
        # Â∞ÜÂÖ∂‰ªñÁöÑagentÁöÑfeature ÊäïÂΩ±Âà∞egoÂùêÊ†áÁ≥ª
        for bid in range(len(memory_batch_lst)):
            N = record_len[bid] # number of valid agent
            
            memory_b = memory_batch_lst[bid] # (N, H*W, C) ÂçïÁã¨‰∏Ä‰∏™Ê†∑Êú¨‰∏ãÁöÑN‰∏™agentÔºåÂÖ∂‰∏≠Á¨¨‰∏Ä‰∏™‰∏∫egoÁöÑfeature
            memory_discrete_b = memory_discrete_batch_lst[bid] # (N, C, H, W) EncoderÁ≠õÈÄâËøáÁöÑÁïô‰∏ãÊù•ÔºåÂÖ∂‰ΩôÂÖ®ÈÉ®‰∏∫Á©∫
            ref_boxes_trans_b = ref_boxes_before_trans_batch_lst[bid][:,:7,:,:] # (N, 7, H, W) EncoderÁ≠õÈÄâËøáÁöÑÁïô‰∏ãÊù•ÔºåÂÖ∂‰ΩôÂÖ®ÈÉ®‰∏∫Á©∫
            ref_probs_trans_b = ref_boxes_before_trans_batch_lst[bid][:,7:,:,:] # (N, 1, H, W) EncoderÁ≠õÈÄâËøáÁöÑÁïô‰∏ãÊù•ÔºåÂÖ∂‰ΩôÂÖ®ÈÉ®‰∏∫Á©∫
            memory_mask_b = memory_mask_batch_lst[bid] # (N, 1, H, W)
            t_matrix = pairwise_t_matrix[bid][:N, :N, :, :] # (N, N, 2, 3)
            t_matrix_ref = pairwise_t_matrix_ref[bid][:N, :N, :, :] # (N, N, 4, 4)

            neighbor_memory = warp_affine_simple(memory_discrete_b, t_matrix[0, :, :, :], (H, W), mode='nearest') # (N, C, H, W)
            ref_boxes_trans_b = warp_affine_simple(ref_boxes_trans_b, t_matrix[0, :, :, :], (H, W), mode='nearest') # (N, 7, H, W)
            neighbor_memory_mask = warp_affine_simple(memory_mask_b, t_matrix[0, :, :, :], (H, W), mode='nearest') # (N, 1, H, W)

            ref_boxes_trans_b = torch.cat([ref_boxes_trans_b, ref_probs_trans_b], dim=1) # (N, 8, H, W)
            neighbor_memory = neighbor_memory.flatten(2).permute(0, 2, 1) # (N, HW, C)
            ref_boxes_trans_b = ref_boxes_trans_b.flatten(2).permute(0, 2, 1) # (N, HW, 8)
            neighbor_memory_mask = neighbor_memory_mask.flatten(2).permute(0, 2, 1) # (N, HW, 1) Ëøô‰∏™ÈáåÈù¢Êúâ0Êúâ1, 1ÁöÑÂú∞ÊñπÂ∞±ÊòØÂØπÂ∫îÂÖ∂ÊúâÊïàÁöÑqueryÔºåËøô‰∫õqueryË¶ÅÂÖàÂú®ego feature‰∏äÂÅöLocal Attention
            # pos_b = src_pos[0:N] # (N, HW, C) NOTE ‰ΩçÁΩÆÁºñÁ†ÅÊØè‰∏™featureÂú®‰∏ÄÂºÄÂßãÊòØÂÆåÂÖ®‰∏ÄÊ†∑ÁöÑ ÊâÄ‰ª•ÂèØ‰ª•Áõ¥Êé•ÂèñÈúÄË¶ÅÁöÑ‰∏™Êï∞

            neighbor_mask = neighbor_memory_mask.squeeze(-1).bool() # (N, HW)
            valid_features_lst = [neighbor_memory[i][neighbor_mask[i]].unsqueeze(0) for i in range(N)] # [(1, n1, C), (1, n2, C)...]
            valid_ref_lst = [ref_boxes_trans_b[i][neighbor_mask[i]].unsqueeze(0) for i in range(N)] # [(1, n1, 8), (1, n2, 8)...]
            record_query_num = torch.tensor([v.size(1) for v in valid_ref_lst]) # [n1, n2, ...]
            # valid_pos_lst = [pos_b[i][neighbor_mask[i]] for i in range(N)] # [(n1, C), (n2, C)...]

            none_ego_features_lst = valid_features_lst[1:] # [(1, n2, C), ...]
            none_ego_ref = valid_ref_lst[1:] # [(1, n2, 8), ...]
            # none_ego_pos = valid_pos_lst[1:]

            none_ego_ref_trans_lst = []
            # ÊóãËΩ¨ÂèÇËÄÉÊ°ÜÔºåÊöÇÊó∂Ê≤°ÊêûÁ©∫Èó¥ÂèòÊç¢Áü©ÈòµÁöÑÁº©ÊîæÔºåÂ¶ÇÊûúÁõ¥Êé•Áº©ÊîæÁ©∫Èó¥ÂèòÊç¢Áü©ÈòµÂàô‰∏çÁî®encodeÂíådecode boxÔºå‰ΩÜÊòØÁõÆÂâçÂÖà‰ª•ËøôÊ†∑ÁöÑÊñπÂºèÈ™åËØÅÈÄªËæë TODO ÂêéÈù¢Ë¶ÅÊîπ
            for id, nef in enumerate(none_ego_ref):
                none_ego_bbox_center = box_decode_func(nef[..., :7].squeeze(0)) # (n, 7) ÂèçÂΩí‰∏ÄÂåñ

                none_ego_bbox_corner = box_utils.boxes_to_corners_3d(none_ego_bbox_center, 'lwh') # (n, 8, 3)
                projected_none_ego_bbox_corner = box_utils.project_box3d(none_ego_bbox_corner.float(), t_matrix_ref[0,id+1].float())
                projected_none_ego_bbox_center = box_utils.corners_to_boxes_3d(projected_none_ego_bbox_corner, 'lwh') # (n, 7)
                projected_none_ego_bbox_center = box_encode_func(projected_none_ego_bbox_center) # ÈáçÊñ∞ÂΩí‰∏ÄÂåñ
                projected_none_ego_bbox_center = torch.cat([projected_none_ego_bbox_center, nef[0, :, 7:]], dim=-1) # # (n, 8)
                none_ego_ref_trans_lst.append(projected_none_ego_bbox_center.unsqueeze(0))
                # ËøòË¶ÅÂ∞ÜÂèòÊç¢ÂêéÁöÑÊîæÂÖ•Âà∞ valid_ref_lst
                valid_ref_lst[id+1] = none_ego_ref_trans_lst[-1]

            if len(none_ego_features_lst) > 0:
                none_ego_features = torch.cat(none_ego_features_lst, dim=1) # (1, n2+n3+..., C)
                none_ego_ref_trans = torch.cat(none_ego_ref_trans_lst, dim=1) # (1, n2+n3+..., 8)
                # none_ego_pos = torch.cat(none_ego_pos, dim=0) # (n2+n3+..., C) # XXX ËÄÉËôë‰∏Ä‰∏ãposÊòØ‰ΩøÁî®refËøòÊòØÁî®egoÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÔºå ÁõÆÂâç‰ΩøÁî®ref‰Ωú‰∏∫posÁºñÁ†Å ÊâÄ‰ª•Ëøô‰∏™ÊöÇÊó∂‰∏çÈúÄË¶Å
            
                # TODO ËøôÈáå‰ªÖ‰ªÖÂØπqueryÂÅö‰∫Ü Local AttentionÔºå‰ΩÜÂπ∂Ê≤°ÊúâÊçÆÊ≠§ÂéªÊõ¥Êñ∞ÊóãËΩ¨ËøáÊù•ÁöÑÂèÇËÄÉÊ°Ü ÊÑüËßâÊòØÈúÄË¶ÅÊõ¥Êñ∞ÁöÑ 
                query_adapt = self.trans_adapter(none_ego_features, memory_b[0:1], src_shape, src_start_index, none_ego_ref_trans) # (1, n2+n3+..., C) ÂÖ∂‰ªñagentÁöÑqueryÂú®ego feature‰∏äËøõË°åLocal Attention

                query_adapt_lst = self.regroup(query_adapt.squeeze(0), record_query_num[1:]) # [(n2, C), ...]

                query_lst = [q.unsqueeze(0) for q in query_adapt_lst]  # [(1, n2, C), ...]
            else: # ÂèØËÉΩÁöÑÊÉÖÂÜµ: 1. Ë∑ùÁ¶ªÂéüÂõ†ÂØºËá¥Âè™Êúâego‰∏Ä‰∏™feature 2. agentÊäïÂΩ±ËøáÊù•Êó†query
                query_lst = []

            query_lst = valid_features_lst[0:1] + query_lst  # [(1, n1, C), (1, n2, C)...]

            all_indices = [] # [(1, n1, 1), (1, n2, 1), (1, n3, 1)...] ‰∏ÄÂÖ±N-1 ‰∏™, Ë°®Á§∫Âú∫ÊôØ‰∏≠ÁöÑÊâÄÊúâÊúâÊïàqueryÁöÑÁ¥¢Âºï ÂÖ∂‰∏≠egoÊàë‰ª¨‰∏çÁî®
            for i in range(N):
                neighbor_index = torch.nonzero(neighbor_memory_mask[i].squeeze(-1), as_tuple=False) # (n, 1)
                if neighbor_index.size(0) > 0:
                    all_indices.append(neighbor_index.unsqueeze(0))
            all_indices[0] = ego_topk_indexes_batch_lst[bid][0:1] # (N, query_num, 1)‰∏≠ÈÄâÊã©Âá∫egoÁöÑ Âç≥(1, query_num, 1)

            ego_feature = memory_b[0:1] # (1, HW, C)

            # Êé•‰∏ãÊù•ÂØπÁõ∏Âêå‰ΩçÁΩÆÁöÑqueryËøõË°åËûçÂêàÔºåagentÊèê‰æõÁöÑÈ¢ùÂ§ñ‰ø°ÊÅØÂàôÊîæÁΩÆÂú®extraÁöÑ‰ΩçÁΩÆ
            if len(all_indices) > 1:
                fused_query, fused_indices = self.fuse_features_by_index(all_indices, query_lst, self.query_fusion, extra_query_batch_lst[bid][0:1], extra_topk_indexes_batch_lst[bid][0:1]) # (1, 300+200, C), (1, 300+200, 1)
                fused_ref, _ = self.fuse_features_by_index(all_indices, valid_ref_lst, self.ref_fusion, extra_topk_proposals_batch_lst[bid][0:1], extra_topk_indexes_batch_lst[bid][0:1]) # (1, 300+200, 8)
                ego_feature = ego_feature.scatter(1, fused_indices.repeat(1, 1, ego_feature.size(-1)), fused_query)
            else: # Â¶ÇÊûúÂà∞ËøôÈáåÔºåÂèØËÉΩÊòØ: 1.Ë∑ùÁ¶ªËøáËøúÂØºËá¥Âè™Êúâ‰∏Ä‰∏™ego 2.agentÊäïÂΩ±ËøáÊù•Êó†query
                fused_query = torch.cat([query_lst[0], extra_query_batch_lst[bid][0:1]], dim=1)
                fused_indices = torch.cat([all_indices[0], extra_topk_indexes_batch_lst[bid][0:1]], dim=1)
                fused_ref = torch.cat([valid_ref_lst[0], extra_topk_proposals_batch_lst[bid][0:1]], dim=1)
                # print("crazy, without overlap! ")
                
            fused_queries.append(fused_query)
            fused_indicies.append(fused_indices)
            fused_ref_windows.append(fused_ref)
            ego_features.append(ego_feature)
        fused_queries = torch.cat(fused_queries, dim=0) # (B, query_num+extra_num, C)
        fused_indicies = torch.cat(fused_indicies, dim=0) # (B, query_num+extra_num, 1)
        fused_ref_windows = torch.cat(fused_ref_windows, dim=0) # (B, query_num+extra_num, 8)
        ego_features = torch.cat(ego_features, dim=0) # (B, HW, C)

        # Âä†Âô™Â£∞gtÔºåÂáÜÂ§á‰∏ÄËµ∑ÂèÇ‰∏édecoderËÆ≠ÁªÉÂéªÂô™
        if noised_gt_box is not None:
            noised_gt_proposals = torch.cat(
                (
                    noised_gt_box,
                    noised_gt_onehot,
                ),
                dim=-1,
            ) # (B, pad_size, 8)
            fused_ref_windows = torch.cat(
                (
                    noised_gt_proposals,
                    fused_ref_windows,
                ),
                dim=1,
            ) # (B, pad_size + all_query_num, 8) while: all_query_num == query_num+extra_num
        init_reference_out = fused_ref_windows[..., :7]

        # hs, inter_references = self.decoder_gt(
        hs, inter_references = self.decoder(
            query_embed, # None
            query_pos, # None
            ego_features, # BoxAttention ÊèêÂèñÁâπÂæÅÂêéÁªìÂêàÂ§öagentÂêéÁöÑFeature Map ÁªìÊûú‰∏∫(B, H*W, 256)
            src_shape, # (1, 2)
            src_start_index, # (0,)
            fused_ref_windows, # (B, all_query_num, 8)
            attn_mask,
        ) # (3, B, pad_size + all_query_num, 256) ÊØè‰∏ÄÂ±ÇÁöÑËæìÂá∫ÁöÑqueryÁâπÂæÅÔºå (3Ôºå B, pad_size + all_query_num, 7) ÊØè‰∏ÄÂ±ÇÁöÑÊ£ÄÊµãÁªìÊûú

        # optional gt forward ÂØπÊØîÂ≠¶‰π†ÈúÄË¶ÅÁî®Âà∞ÁöÑÂä®ÈáèÊõ¥Êñ∞Ê®°ÂûãÁî®Âä†Âô™gtÊù•ÂÅöÂØπÊØîÂ≠¶‰π†ÁöÑ
        if targets is not None:
            batch_size = len(targets) # ËøôÈáåÊòØÂçèÂêåÊ†áÁ≠æ
            per_gt_num = [tgt["gt_boxes"].shape[0] for tgt in targets] # [N1, N2, N3, N4] Ê≠§‰∏∫B=4Êó∂ÁöÑÂêÑ‰∏™Ê†∑Êú¨ÁöÑGTÊï∞
            max_gt_num = max(per_gt_num)
            batched_gt_boxes_with_score = memory.new_zeros(batch_size, max_gt_num, 8) # (B, max_gt_num, 8)
            for bi in range(batch_size):
                batched_gt_boxes_with_score[bi, : per_gt_num[bi], :7] = targets[bi]["gt_boxes"] # ÊîæÂÖ•gtÁöÑbox Âíå one-hot ÂàÜÁ±ªÁºñÁ†Å
                batched_gt_boxes_with_score[bi, : per_gt_num[bi], 7:] = F.one_hot(
                    targets[bi]["labels"], num_classes=self.num_classes
                )

            with torch.no_grad():
                self._momentum_update_gt_decoder() # Âä®ÈáèÊõ¥Êñ∞ËæÖÂä©Ê®°ÂûãÔºåÂÖ∂ÂèÇÊï∞Êõ¥Êñ∞ÈÄüÂ∫¶ÈùûÂ∏∏ÁºìÊÖ¢Ôºå‰ΩÜ‰∏ÄÁõ¥ËøΩÈöèdecoder
                if noised_gt_box is not None:
                    dn_group_num = noised_gt_proposals.shape[1] // (max_gt_num * 2) # ÂæóÂà∞ÂéªÂô™gtÁªÑÊï∞ == 3  2ÊåáÁöÑÊòØÊØè‰∏ÄÁªÑÂèàÂàÜÊ≠£Ë¥üÊ†∑Êú¨
                    pos_idxs = list(range(0, dn_group_num * 2, 2))
                    pos_noised_gt_proposals = torch.cat(
                        [noised_gt_proposals[:, pi * max_gt_num : (pi + 1) * max_gt_num] for pi in pos_idxs],
                        dim=1,
                    ) # ÊØè‰∏ÄÁªÑÊäΩÂèñmax_gt_num‰∏™ (B, 3*max_gt_num, 8) ËøôÊòØÁõ∏ÂΩì‰∫éÂéªÂô™Ê≠£Ê†∑Êú¨ÊäΩÂèñÂá∫Êù•
                    gt_proposals = torch.cat((batched_gt_boxes_with_score, pos_noised_gt_proposals), dim=1)
                    # create attn_mask for gt groups
                    gt_attn_mask = memory.new_ones(
                        (dn_group_num + 1) * max_gt_num, (dn_group_num + 1) * max_gt_num
                    ).bool()  # Ôºà4*max_gt_numÔºå4*max_gt_numÔºâÂÖ®True
                    for di in range(dn_group_num + 1): # ÂØπËßíÈÉ®ÂàÜmask ÂÖ®ÈÉ®ËÆæÁΩÆ‰∏∫FalseÔºåÁõ∏ÂΩì‰∫éËØ¥Âè™ÂÖ≥Ê≥®Ëá™Â∑±ÔºåÂç≥ÊØè‰∏ÄÊâπgtÔºåÊó†ËÆ∫ÊúâÊó†Âô™Â£∞Ôºå‰ªÖÂÖ≥Ê≥®Ëá™Ë∫´ÔºåÂ±èËîΩÁªÑ‰πãÈó¥ÁöÑÂèØËßÅÊÄß
                        gt_attn_mask[
                            di * max_gt_num : (di + 1) * max_gt_num,
                            di * max_gt_num : (di + 1) * max_gt_num,
                        ] = False
                else:
                    gt_proposals = batched_gt_boxes_with_score
                    gt_attn_mask = None

                hs_gt, inter_references_gt = self.decoder_gt( # ËæÖÂä©Ê®°ÂûãËøõË°åÂØπÊØîÂ≠¶‰π†ÔºåÁºìÊÖ¢ËøΩÈöèdecoder„ÄÇ ËøîÂõû (3ÔºåB, 4*max_gt_num, 256) ‰∏é (3ÔºåB, 4*max_gt_num, 8)
                    None,
                    None,
                    ego_features, # BoxAttention ÊèêÂèñÁâπÂæÅÂêéÁªìÂêàÂ§öagentÂêéÁöÑFeature Map ÁªìÊûú‰∏∫(B, H*W, 256)
                    src_shape, # (1, 2)
                    src_start_index, # (0,)
                    gt_proposals, # (B, 4*max_gt_num, 8)
                    gt_attn_mask, #Ôºà4*max_gt_numÔºå4*max_gt_numÔºâ
                )

            init_reference_out = torch.cat(
                (
                    init_reference_out,
                    gt_proposals[..., :7],
                ),
                dim=1,
            ) # (B, pad_size + all_query_num + 4*max_gt_num, 8) while: all_query_num == query_num+extra_num ËæìÂÖ•decoderÂâçÁöÑref window

            hs = torch.cat(
                (
                    hs,
                    hs_gt,
                ),
                dim=2,
            ) # (3, B, pad_size + all_query_num + 4*max_gt_num, 256) ÊØè‰∏ÄÂ±ÇDecoder layerÁöÑËæìÂá∫query
            inter_references = torch.cat(
                (
                    inter_references,
                    inter_references_gt,
                ),
                dim=2,
            ) # (3Ôºå B, pad_size + all_query_num + 4*max_gt_num, 7) ÊØè‰∏ÄÂ±ÇDecoder layerÁöÑÂØπÂ∫îÊ£ÄÊµãÁªìÊûú

        inter_references_out = inter_references
        '''
        ‰ªéÂâçÂæÄÂêé‰æùÊ¨°ËøîÂõû: Decoder layerÊØè‰∏ÄÂ±ÇÁöÑquery, ËæìÂÖ•DecoderÁöÑÂèÇËÄÉÊ°Ü, Decoder layerÊØè‰∏ÄÂ±ÇÁöÑÊ£ÄÊµãÁªìÊûú, EncoderËæìÂá∫ÁöÑÁâπÂæÅÂõæ, ÂàùÂßãÂåñÁöÑÂèÇËÄÉÊ°Ü, egoÁöÑÊúÄÈ´òquery_numÁöÑÁ¥¢Âºï
        TODO EncoderËæìÂá∫ÁöÑÁâπÂæÅÂõæ‰ø°ÊÅØ‰ºö‰∏ç‰ºö‰∏çË∂≥? Ë¶Å‰∏çË¶ÅËÄÉËôëÂ∞ÜqueryËûçÂêàÂêéÁöÑ‰ø°ÊÅØÊîæÂõûÂéª üåüUpdated: Done, ÂÖàÁúãÁúãÊÄßËÉΩ
        '''
        return hs, init_reference_out, inter_references_out, memory, src_anchors, ego_topk_indexes

    def fuse_features_by_index(self, index_list, feature_list, fusion_func, extra_future, extra_index):
        """
        Ê†πÊçÆÁ¥¢ÂºïÂØπÁâπÂæÅËøõË°åËûçÂêà„ÄÇ

        ÂèÇÊï∞:
        - index_list: list of torch.Tensor, ÂΩ¢Áä∂‰∏∫ (1, n, 1) ÁöÑÁ¥¢ÂºïÂº†ÈáèÂàóË°®ÔºåÊØè‰∏™Ë°®Á§∫ÊúâÊïàÁöÑÁ¥¢Âºï‰ΩçÁΩÆ„ÄÇ eg. [(1,300,1), (1,62,1)...]
        - feature_list: list of torch.Tensor, ÂΩ¢Áä∂‰∏∫ (1, n, C) ÁöÑÁâπÂæÅÂõæÂº†ÈáèÂàóË°®„ÄÇ  eg. [(1,300,C), (1,62,C)...]
        - fusion_func: Callable, Ëá™ÂÆö‰πâËûçÂêàÂáΩÊï∞, Êé•ÂèóËæìÂÖ• (n, k, C)ÔºåËøîÂõûËûçÂêàÂêéÁöÑÂº†Èáè (n, 1, C),
                    ÂÖ∂‰∏≠ k Ë°®Á§∫ÂèÇ‰∏éËûçÂêàÁöÑÁâπÂæÅÊï∞Èáè„ÄÇ
        - extra_future: (1, 200, C), egoËá™Ë∫´refine‰∫Ü500‰∏™query, ÂÖ∂‰∏≠300‰∏™ÂèÇ‰∏éËûçÂêà, Âêé200‰∏™Áî®‰∫é‰ªéÂâçÂà∞ÂêéÂ°´ÂÖÖ‰∏çÈáçÂè†ÁöÑÂÖ∂‰ªñagentÁöÑquery 
        - extra_index: (1, 200, 1)

        ËøîÂõû:
        - fused_features: torch.Tensor, ËûçÂêàÂêéÁöÑÁâπÂæÅÂº†Èáè, ÂΩ¢Áä∂‰∏∫ (1, ego_query_num + extra_query_num, C)„ÄÇ  eg. (1, 300+200, C)
        """
        # Ê£ÄÊü•ËæìÂÖ•ÂêàÊ≥ïÊÄß
        assert len(index_list) == len(feature_list), "Á¥¢ÂºïÂàóË°®ÂíåÁâπÂæÅÂõæÂàóË°®ÈïøÂ∫¶‰∏ç‰∏ÄËá¥"
        
        # Áªü‰∏ÄÂ§ÑÁêÜÁ¥¢ÂºïÔºåËé∑ÂèñÊâÄÊúâÂîØ‰∏ÄÁ¥¢Âºï
        all_indices = torch.cat([idx.squeeze(0) for idx in index_list], dim=0)  # (sum(n), 1)
        # Áõ∏ÂêåÁöÑÁ¥¢ÂºïÊÑèÂë≥ÁùÄÁõ∏ÂêåÁöÑ‰ΩçÁΩÆ, (n_unique, ) ÂíåÈÄÜÊò†Â∞Ñ (sum(n),) Ë°®Á§∫ÊØè‰∏™ÂÖÉÁ¥†Âú®unique_indices‰∏≠ÁöÑ‰ΩçÁΩÆ
        # FIXME ‰ªÄ‰πàÊÉÖÂÜµ? Âç≥‰ΩøËÆæÁΩÆ‰∏çÁî®ÊéíÂ∫èÔºå‰ΩÜÊòØÊúÄÂêéÁªìÊûú‰æùÁÑ∂ÊéíÂ∫èÔºåÊÉ≥Ë¶ÅÁ®≥ÂÆöÂéªÈáçÔºåÂè™ËÉΩËá™Â∑±ÂÜôÊ±Çunique
        # unique_indices, inverse_indices = torch.unique(all_indices, sorted=False, return_inverse=True) 

        seen = set()
        unique_vals = []
        for val in all_indices:
            scalar_val = val.item() # ËøôÈáådebug‰∫ÜÂ•Ω‰πÖÔºåtensorÂØπË±°ÊòØ‰∏çÂèØÂìàÂ∏åÁöÑÔºåÊ≤°ÊêûÊòéÁôΩÁõ¥Êé•ÂØºËá¥ËøôÈáåÂéªÈáçÂ§±Ë¥•ÔºåËøò‰ºöÂá∫Áé∞ÈáçÂ§çÔºåÂõ†Ê≠§ÂøÖÈ°ªËΩ¨‰∏∫pythonÊ†áÈáè
            if scalar_val not in seen:
                seen.add(scalar_val)
                unique_vals.append(scalar_val)
        unique_indices = torch.tensor(unique_vals).to(all_indices)

        # ÊûÑÂª∫ÊØè‰∏™Á¥¢ÂºïÂØπÂ∫îÁöÑÁâπÂæÅÂàóË°®
        feature_map = {idx.item(): [] for idx in unique_indices} # eg. {id: [(1, C), ...]}
        for idx, features in zip(index_list, feature_list):
            for i, ind in enumerate(idx.squeeze(0).squeeze(-1)): # ÈÅçÂéÜÊØè‰∏™agentÁöÑÁ¥¢Âºï
                feature_map[ind.item()].append(features[:, i, :])  # ÊåâÁ¥¢ÂºïÂ≠òÂÖ•ÁâπÂæÅ (1, C)

        # ÂØπÊØè‰∏™ÂîØ‰∏ÄÁ¥¢ÂºïËøõË°åËûçÂêà ÁÑ∂ÂêéÈáçÊñ∞ÊîæÂõûÂéª ÂΩ¢Êàê{unique_id: [feature]}
        fused_features = []  # Â≠òÂÇ®ËûçÂêàÂêéÁöÑÁâπÂæÅ
        for idx in unique_indices:
            features_to_fuse = torch.stack(feature_map[idx.item()], dim=1)  # (1, k, C) Âêå‰∏Ä‰∏™Á©∫Èó¥‰ΩçÁΩÆÊúâÂ§ö‰∏™feature, ÂèØËÉΩÊòØegoÂíåÂÖ∂‰ªñagentÔºå‰πüÂèØËÉΩÊòØagent‰πãÈó¥
            fused_features.append(fusion_func(features_to_fuse)) # ËûçÂêàËøîÂõûÁöÑÂ∫îËØ•ÊòØ(1, 1, C)
        fused_features = torch.cat(fused_features, dim=1)  # (1, n_unique, C)

        # ‰ªé fused_features ‰∏≠ÊèêÂèñÂ±û‰∫é ego ÁöÑÁâπÂæÅ
        ego_indices = index_list[0].squeeze(0).squeeze(-1)  # ego ÁöÑÁ¥¢Âºï Ôºàn1,Ôºâ egoÁöÑÁ¥¢Âºï‰∏™Êï∞ÊòØÂõ∫ÂÆöÁöÑÔºåÂ∞±Á≠â‰∫équery_num
        ego_mask = torch.isin(unique_indices, ego_indices)  # ÊâæÂà∞Â±û‰∫é ego ÁöÑÁ¥¢Âºï (n_unique, ) egoÂØπÂ∫îÁöÑÁ¥¢ÂºïÂ∞±‰∏∫ True
        ego_features = fused_features[:, ego_mask, :]  # ÊèêÂèñÂ±û‰∫é ego ÁöÑÈÉ®ÂàÜ (1, ego_query_size, C)

        non_overlap_features = []
        for idx, features in zip(index_list[1:], feature_list[1:]): # ÂøΩÁï• ego
            mask = ~torch.isin(idx.squeeze(0), index_list[0].squeeze(0)) # ÈùûÈáçÂè†ÈÉ®ÂàÜ (n_unique, 1) XXX È¶ñÂÖàÂÆåÂÖ®ÈáçÂè†‰∏çÂèØËÉΩÔºåÈÇ£Âè™Êúâ‰∏ÄÁßçÂèØËÉΩÔºåÈÇ£Â∞±ÊòØagentÂíåegoÊÑüÁü•ËåÉÂõ¥ÈÉΩ‰∏çÈáçÂêàÔºåÊâÄ‰ª•Ê†πÊú¨Â∞±ÊòØÁ©∫
            selected_features = features[:, mask.squeeze(), :] # ÊèêÂèñÈùûÈáçÂè†ÁâπÂæÅ (1, k', C)
            if selected_features.size(1) > 0:
                non_overlap_features.append(selected_features)

        # Â∞ÜÈùûÈáçÂè†ÁâπÂæÅÊåâÂàÜÊï∞Êà™Êñ≠Âπ∂Â°´ÂÖÖÂà∞ÊúÄÁªàÁªìÊûú‰∏≠
        if len(non_overlap_features) > 0:
            non_overlap_features = torch.cat(non_overlap_features, dim=1)  # (1, k_all, C)
            append_num = min(non_overlap_features.size(1), self.extra_query_num) # ÊúÄÂ§ß‰∏çË∂ÖËøá extra_query_num
            extra_future[:, :append_num, :] = non_overlap_features[:,:append_num,:]
        # else: # È¶ñÂÖàËÉΩËøõÂÖ•ËûçÂêàÂáΩÊï∞Â∞±ËØ¥ÊòéÊúâÊäïÂΩ±ÁöÑqueryÂ≠òÂú®ÔºåÁªìÊûúÈùûÈáçÂè†ÁöÑÁâπÂæÅÊòØ0ÔºåËøôÂ∞±ËØ¥ÊòéÂÖ®ÈÉ®ÊòØÈáçÂè†ÁöÑÁâπÂæÅ, ÁªèËøáÈ™åËØÅÔºåÊ≠§Êó∂ÊäïÂΩ±ËøáÊù•ÁöÑÁâπÂæÅÊï∞ÈáèÂæàÂ∞ëÔºå‰∏ÄËà¨ÊòØ‰∏™‰ΩçÊï∞ÔºåÊûÅÂ∞ëÊï∞Êó∂ÂÄôÊòØÂá†ÂçÅ
        #     print("------------------------------------------------")
        #     print("Oops! All overlap???")
        #     print("unique_indices shape is ", unique_indices.shape)
        #     print("agent 1 shape is ", index_list[1].shape)
        #     print("------------------------------------------------")

        # ÊúÄÁªàÁâπÂæÅ: ego + extra_future
        final_features = torch.cat([ego_features, extra_future], dim=1)  # (1, ego_query_size + etra_query_num, C)

        unique_indices = unique_indices.unsqueeze(0).unsqueeze(-1) # (1, n_unique, 1)
        index_num = min(unique_indices.size(1), self.num_queries + self.extra_query_num)
        assert unique_indices.size(1) >= self.num_queries
        remain_start = index_num - self.num_queries
        final_indices = torch.cat([unique_indices[:, :index_num, :], extra_index[:, remain_start:, :]], dim = 1) # 500
        return final_features, final_indices

    
# simple fusion use Scaled Dot Product Attention
class AttenQueryFusion(nn.Module):
    def __init__(self, feature_dim):
        super(AttenQueryFusion, self).__init__()
        self.sqrt_dim = np.sqrt(feature_dim)

    def forward(self, x):
        # x : 1, k, C
        if x.size(1) == 1:
            return x
        query = key = value = x
        score = torch.bmm(query, key.transpose(1, 2)) / self.sqrt_dim
        attn = F.softmax(score, -1)
        context = torch.bmm(attn, value) # (1,k,C)
        x = context[:,0:1,:]
        return x

class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, nlevel, dim_feedforward, dropout, activation):
        super().__init__()
        self.self_attn = Box3dAttention(d_model, nlevel, nhead, with_rotation=False)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        self.activation = get_activation_fn(activation)

    @staticmethod
    def with_pos_embed(tensor, pos):
        return tensor if pos is None else tensor + pos

    def forward(self, src, pos, src_shape, src_start_idx, ref_windows):
        src2 = self.self_attn(
            self.with_pos_embed(src, pos),
            src,
            src_shape,
            None,
            src_start_idx,
            None,
            ref_windows,
        )

        src = src + self.dropout1(src2[0])
        src = self.norm1(src)

        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)

        return src


class TransformerEncoder(nn.Module):
    def __init__(self, d_model, encoder_layer, num_layers):
        super().__init__()
        self.layers = get_clones(encoder_layer, num_layers)

    def forward(self, src, pos, src_shape, src_start_idx, ref_windows):
        output = src
        for layer in self.layers:
            output = layer(output, pos, src_shape, src_start_idx, ref_windows)
        return output


class TransformerDecoderLayer(nn.Module):
    def __init__(self, d_model, nhead, nlevel, dim_feedforward, dropout, activation):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.multihead_attn = Box3dAttention(d_model, nlevel, nhead, with_rotation=True)

        self.pos_embed_layer = MLP(8, d_model, d_model, 3)

        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)
        self.dropout = nn.Dropout(dropout)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        self.activation = get_activation_fn(activation)

    @staticmethod
    def with_pos_embed(tensor, pos):
        return tensor if pos is None else tensor + pos

    def forward(self, idx, query, query_pos, memory, memory_shape, memory_start_idx, ref_windows, attn_mask=None):
        if idx == 0:
            query = self.pos_embed_layer(ref_windows)
            q = k = query
        elif query_pos is None:
            query_pos = self.pos_embed_layer(ref_windows)
            q = k = self.with_pos_embed(query, query_pos)

        q = q.transpose(0, 1)
        k = k.transpose(0, 1)
        v = query.transpose(0, 1)

        query2 = self.self_attn(q, k, v, attn_mask=attn_mask)[0]
        query2 = query2.transpose(0, 1)
        query = query + self.dropout1(query2)
        query = self.norm1(query)

        query2 = self.multihead_attn(
            self.with_pos_embed(query, query_pos),
            memory,
            memory_shape,
            None,
            memory_start_idx,
            None,
            ref_windows[..., :7],
        )[0]

        query = query + self.dropout2(query2)
        query = self.norm2(query)

        query2 = self.linear2(self.dropout(self.activation(self.linear1(query))))
        query = query + self.dropout3(query2)
        query = self.norm3(query)

        return query


class TransformerDecoder(nn.Module):
    def __init__(self, d_model, decoder_layer, num_layers, cp_flag):
        super().__init__()

        self.layers = get_clones(decoder_layer, num_layers)
        self.cp_flag = cp_flag # True
        if self.cp_flag:
            print("===‰ΩøÁî®checkpoint‰ºòÂåñÂÜÖÂ≠ò, ‰ΩÜÊòØ‰ºöÈôç‰ΩéËÆ≠ÁªÉÈÄüÂ∫¶===")

    def forward(self, query, query_pos, memory, memory_shape, memory_start_idx, ref_windows, attn_mask=None):
        output = query
        intermediate = []
        intermediate_ref_windows = []
        for idx, layer in enumerate(self.layers):
            if self.cp_flag:
                output = cp.checkpoint(layer, idx, output, query_pos, memory, memory_shape, memory_start_idx, ref_windows, attn_mask)
            else:
                output = layer(idx, output, query_pos, memory, memory_shape, memory_start_idx, ref_windows, attn_mask)
            new_ref_logits, new_ref_windows = self.detection_head(output, ref_windows[..., :7], idx)
            new_ref_probs = new_ref_logits.sigmoid()
            ref_windows = torch.cat(
                (
                    new_ref_windows.detach(),
                    new_ref_probs.detach(),
                ),
                dim=-1,
            )
            intermediate.append(output)
            intermediate_ref_windows.append(new_ref_windows)
        return torch.stack(intermediate), torch.stack(intermediate_ref_windows)

class TransAdapt(nn.Module):
    def __init__(self, d_model, nhead, nlevel, dim_feedforward, dropout, activation):
        super().__init__()
        self.multihead_attn = Box3dAttention(d_model, nlevel, nhead, with_rotation=True)

        self.pos_embed_layer = MLP(8, d_model, d_model, 3)

        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        self.activation = get_activation_fn(activation)

    @staticmethod
    def with_pos_embed(tensor, pos):
        return tensor if pos is None else tensor + pos

    def forward(self, query, memory, memory_shape, memory_start_idx, ref_windows):
        '''
        query: (1, n, C)
        memory: (1, HW, C)
        memory_shape: (1,2)
        memory_start_idx: (0,)
        ref_windows: (1, n, 8)
        '''
        if query.size(1) == 0: # Â¶ÇÊûúÂÖ∂‰ªñagentÁöÑqueryÊï∞ÊòØ0ÔºåÈÇ£Â∞±Áõ¥Êé•returnÂç≥ÂèØ
            return query
        query_pos = self.pos_embed_layer(ref_windows)

        query2 = self.multihead_attn(
            self.with_pos_embed(query, query_pos),
            memory,
            memory_shape,
            None,
            memory_start_idx,
            None,
            ref_windows[..., :7],
        )[0]

        query = query + self.dropout2(query2)
        query = self.norm2(query)

        query2 = self.linear2(self.dropout(self.activation(self.linear1(query))))
        query = query + self.dropout3(query2)
        query = self.norm3(query)

        return query


def get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])


def get_activation_fn(activation):
    """Return an activation function given a string"""
    if activation == "relu":
        return F.relu
    if activation == "gelu":
        return F.gelu
    if activation == "glu":
        return F.glu
    raise RuntimeError(f"activation should be relu/gelu, not {activation}.")


def flatten_with_shape(tensor_list, mask_list):
    """
    Params:
    :tensor_list: [(B, C, H1, W1), ..., (B, C, HN, WN)]
    :mask_list: [(B, H1, W1), ..., (B, HN, WN)]

    Return:
    :tensor_flatten: (B, L, C)
    :mask_flatten: (B, L)
    :tensor_shape: (N, 2)
    """
    assert isinstance(tensor_list, collections.abc.Sequence)
    assert len(tensor_list) > 0

    N = len(tensor_list) # 1
    tensor_shape = torch.zeros(N, 2, dtype=torch.int64, device=tensor_list[0].device) # (1, 2)
    tensor_flatten = []

    if mask_list is not None:
        mask_flatten = []

    for i, tensor in enumerate(tensor_list):
        new_tensor = tensor.flatten(2).permute(0, 2, 1) # Â±ïÂπ≥ÊàêÔºàBÔºåH*WÔºåCÔºâ
        tensor_flatten.append(new_tensor)

        if mask_list is not None:
            mask = mask_list[i]
            new_mask = mask.flatten(1)
            mask_flatten.append(new_mask)
            assert tensor.shape[2] == mask.shape[1]
            assert tensor.shape[3] == mask.shape[2]
        tensor_shape[i, 0] = tensor.shape[2]
        tensor_shape[i, 1] = tensor.shape[3]

    mask_flatten = torch.cat(mask_flatten, dim=1) if mask_list is not None else None # (B, L)
    tensor_flatten = torch.cat(tensor_flatten, dim=1)

    return tensor_flatten, mask_flatten, tensor_shape