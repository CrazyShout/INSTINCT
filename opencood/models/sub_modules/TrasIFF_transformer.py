import collections
import copy

import torch
from torch import nn
from torch.nn import functional as F
import torch.utils.checkpoint as cp
import numpy as np
import math

from opencood.models.sub_modules.box_attention import Box3dAttention
from opencood.models.sub_modules.torch_transformation_utils import warp_affine_simple
from opencood.utils import box_utils

class MLP(nn.Module):
    """Very simple multi-layer perceptron (also called FFN)"""

    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super().__init__()
        self.num_layers = num_layers # 3
        h = [hidden_dim] * (num_layers - 1) # [256, 256]
        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)
        return x
    
class FeatureMagnet(nn.Module):
    def __init__(self, d_model=256, nhead=8,  dim_feedforward=1024, dropout=0.1, activation="relu"):
        super().__init__()

        self.query_fusion = MLP(d_model*2, d_model, d_model, 3)

        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)

        self.activation = get_activation_fn(activation)
    
    def forward(self, x, pos_1d, ego_idx=0, ref_windows=None):
        '''
        x: [(1, L1, C), (1, L2, C)...] egoå’Œå…¶ä»–agentçš„query, è¿™äº›queryå·²ç»è¢«CDAæ¨¡å—å¤„ç†è¿‡äº†
        pos_1d: [(1, L1, 1), (1, L2, 1)...] æ ‡è®°äº†åœ¨ç»Ÿä¸€åæ ‡ç³»ä¸‹æ¯ä¸ªå¯¹åº”queryçš„ä½ç½®, è¿™ä¸ªä½ç½®å…·æœ‰å”¯ä¸€æ€§ ä½†æ˜¯ç”±äºå…¶ä»–agentæ—‹è½¬åå–æ•´, å¯èƒ½é€ æˆ1dç¼–ç ç›¸åŒ
        ref_windows: [(1, L1, 7), (1, L2, 7)...]  # å‚è€ƒæ¡†, ç”¨æ¥BoxAttention
        '''
        B_N = x[0].shape[0]  # Batch size
        final_queries = []  # To hold final queries
        
        # Process each agent and the ego agent separately
        ego_queries = x[ego_idx] # (1, L1, C)
        ego_positions = pos_1d[ego_idx] # (1, L1, 1)
        
        # Collect all other agents' queries and positions
        other_agents_queries = x[:ego_idx] + x[ego_idx+1:]
        other_agents_positions = pos_1d[:ego_idx] + pos_1d[ego_idx+1:]
        
        # 1. For the ego agent, we want to collect the queries based on position matching
        ego_position_queries = {}  # Dictionary to hold queries by position
        for i in range(ego_queries.shape[1]): # éå†æ¯ä¸ªå‘é‡
            pos = ego_positions[0, i].item()  # Get the position (assuming batch_size=1 for simplicity)
            if pos not in ego_position_queries:
                ego_position_queries[pos] = []
            ego_position_queries[pos].append(ego_queries[:, i, :]) # poså’Œfeature æ˜¯å¯¹åº”çš„ï¼Œè¿™é‡Œç›´æ¥å°†(1,C)æ”¾å…¥
        
        # 2. For each other agent, we process its queries based on the positions
        for agent_queries, agent_positions in zip(other_agents_queries, other_agents_positions):
            agent_position_queries = {}  # Dictionary to hold queries by position
            for i in range(agent_queries.shape[1]):
                pos = agent_positions[0, i].item()  # Get the position
                if pos not in agent_position_queries:
                    agent_position_queries[pos] = []
                agent_position_queries[pos].append(agent_queries[:, i, :]) # åŒä¸€ä¸ªä½ç½®çš„æ”¾å…¥ä¸€ä¸ªåˆ—è¡¨é‡Œï¼Œæ³¨æ„è¿™é‡Œå¯èƒ½ç”±äºæ—‹è½¬é€ æˆä½ç½®é‡å 
            
            # 3. Fuse the agent's queries with the ego's queries at the same position
            for pos, agent_query_list in list(agent_position_queries.items()): # liståˆ›å»ºå‰¯æœ¬ 
                if pos in ego_position_queries: # å¦‚æœåœ¨egoä¸­ä¹Ÿæœ‰é‡å¤çš„ä½ç½® 
                    # Fuse the queries using MLP (aggregation of queries at the same position)
                    sum_tensor = sum(agent_query_list) # ä»¥é˜²æ‰­æ›²åçš„é‡å æŠ•å½±ï¼Œä½ç½®ç›¸åŒçš„å°±ç›´æ¥ç›¸åŠ åœ¨ä¸€èµ· TODO ç›´æ¥ç›¸åŠ æ˜¯å¦åˆç†ï¼Ÿå¦‚æœç”¨åæ ‡ç½‘æ ¼æ¥å®ç°TransIFFï¼Ÿ
                    ego_position_queries[pos] += [sum_tensor] # [(1,C), (1,C)]

                    agent_position_queries.pop(pos)

                    # agent_queries_fused = torch.cat(agent_query_list, dim=1)  # Concatenate queries for the same position
                    ego_queries_fused = torch.cat(ego_position_queries[pos], dim=1)  # Concatenate ego queries for the same position (1, C*2)
                    # Perform fusion
                    fused_queries = self.query_fusion(ego_queries_fused)  # MLP fusion (1, C)
                    # Update the ego's query with the fused result
                    ego_position_queries[pos] = [fused_queries]  # Update with the fused queries
                else: # å¦‚æœæ²¡æœ‰é‡å ä¹Ÿè¦ä¿è¯æ‰€æœ‰æŠ•å½±åçš„ä½ç½®å”¯ä¸€ï¼Œé‡å çš„queryå°±ç›¸åŠ åœ¨ä¸€èµ·
                    sum_tensor = sum(agent_query_list)
                    agent_position_queries[pos] = [sum_tensor]
        
        # Now combine the final queries for ego and other agents é‡æ–°æ¢å¤æˆ (1, l1, C)
        final_ego_queries = torch.cat([val[0] for val in ego_position_queries.values()], dim=0).unsqueeze(0)  # Concatenate all position-based fused queries
        final_queries.append(final_ego_queries)  # Add Ego's final queries

        agent_remain_queries = torch.cat([val[0] for val in agent_position_queries.values()], dim=0).unsqueeze(0) # (1, l2', C)
        final_queries.append(agent_remain_queries)  # Add Ego's final queries

        # Stack all the final queries together
        final_queries = torch.cat(final_queries, dim=1)  # Concatenate queries from ego and agents (1, l1+l2', C) è¿™ä¸ªä¹Ÿå°±æ˜¯paperä¸­æåˆ°çš„ optimal Q
        # print("final_queries shape is ", final_queries.shape)

        k = v =  torch.cat(x, dim=1) # åŸå§‹çš„ æ‰€æœ‰agent ç­›é€‰çš„query feature å…¨éƒ¨concat åœ¨ä¸€èµ·ï¼Œå½¢æˆ(1, l1+l2, C)
        # print("k shape is ", k.shape)

        outputs = self.self_attn(final_queries, k, v)[0]
        final_queries = final_queries + self.dropout(outputs)
        final_queries = self.norm1(final_queries)
        outputs = self.linear2(self.dropout(self.activation(self.linear1(final_queries))))
        final_queries = final_queries + self.dropout(outputs)
        final_queries = self.norm2(final_queries)

        return final_queries # (1, L, C) æœ€ç»ˆå°±æ˜¯è¾“å‡ºè¿™ä¸ªLä¸ªquery


class CrossDomainAdaption(nn.Module):
    def __init__(self, d_model=256, nhead=8,  dim_feedforward=1024, dropout=0.1, activation="relu"):
        super().__init__()
        self.self_attn_ego = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)
        self.self_attn_inf = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)
        self.ego_linear1 = nn.Linear(d_model, dim_feedforward)
        self.ego_linear2 = nn.Linear(dim_feedforward, d_model)
        self.ego_norm1 = nn.LayerNorm(d_model)
        self.ego_norm2 = nn.LayerNorm(d_model)

        self.inf_linear1 = nn.Linear(d_model, dim_feedforward)
        self.inf_linear2 = nn.Linear(dim_feedforward, d_model)
        self.inf_norm1 = nn.LayerNorm(d_model)
        self.inf_norm2 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)

        self.activation = get_activation_fn(activation)

    @staticmethod
    def with_pos_embed(tensor, pos):
        return tensor if pos is None else tensor + pos
    
    def forward(self, x, pos):
        '''
        x: [(1, L1, C), (1, L2, C)...] egoå’Œå…¶ä»–agentçš„ç­›é€‰è¿‡çš„query egoæœ‰L1ä¸ª agent1æœ‰L2ä¸ª ä»¥æ­¤ç±»æ¨
        pos: [(1, L1, C), (1, L2, C)...] ä½ç½®ç¼–ç , ç”¨çš„æ˜¯ç©ºé—´å˜æ¢è¿‡çš„ä½ç½®ç¼–ç 
        é¦–å…ˆæ˜¯åŠ ä½ç½®ç¼–ç , åŠ äº†ä½ç½®ç¼–ç åconcatåœ¨ä¸€èµ·å½¢æˆ
        '''
        features = []
        for i, feat in enumerate(x):
            feat = self.with_pos_embed(feat, pos[i])
            features.append(feat)
        k =v = torch.cat(features, dim=1) # (1, L1+L2+..., C)
        assert len(x) == 2 # TransIFF æ˜¯ä¸“é—¨é¢å‘V2Iè®¾è®¡
        q_ego, q_inf = features[0], features[1]

        # two-stream
        q_ego_2 = self.self_attn_ego(q_ego, k, v)[0]
        q_ego = q_ego + self.dropout(q_ego_2)
        q_ego = self.ego_norm1(q_ego)
        q_ego_2 = self.ego_linear2(self.dropout(self.activation(self.ego_linear1(q_ego))))
        q_ego = q_ego + self.dropout(q_ego_2)
        q_ego = self.ego_norm2(q_ego)

        q_inf_2 = self.self_attn_inf(q_inf, k, v)[0]
        q_inf = q_inf + self.dropout(q_inf_2)
        q_inf = self.inf_norm1(q_inf)
        q_inf_2 = self.inf_linear2(self.dropout(self.activation(self.inf_linear1(q_inf))))
        q_inf = q_inf + self.dropout(q_inf_2)
        q_inf = self.ego_norm2(q_inf)

        return [q_ego, q_inf]


class TransIFFEncoder(nn.Module):
    def __init__(
        self,
        d_model=256,
        nhead=8,
        nlevel=1,
        num_encoder_layers=1,
        num_decoder_layers=1,
        dim_feedforward=1024,
        dropout=0.1,
        activation="relu",
        num_queries=300,
        num_classes=1,
        mom=0.999,
        cp_flag=False,
    ):
        super().__init__()

        self.num_queries = num_queries
        self.num_classes = num_classes
        self.m = mom

        encoder_layer = TransformerEncoderLayer(d_model, nhead, nlevel, dim_feedforward, dropout, activation)
        self.encoder = TransformerEncoder(d_model, encoder_layer, num_encoder_layers)
        # decoder_layer = TransformerDecoderLayer(d_model, nhead, nlevel, dim_feedforward, dropout, activation)
        # self.decoder = TransformerDecoder(d_model, decoder_layer, num_decoder_layers)

    def _generate_relative_position_encoding(self, H, W):
        # åˆ›å»ºä¸€ä¸ªç½‘æ ¼ï¼Œå…¶ä¸­æ¯ä¸ªä½ç½®çš„ (i, j) åæ ‡
        grid_x, grid_y = torch.meshgrid(torch.arange(H), torch.arange(W))
        
        # å°†ä½ç½®ç¼–ç å †å æˆ (H, W, 2) çš„å½¢çŠ¶
        position_encoding = torch.stack((grid_x, grid_y), dim=-1)
        
        return position_encoding

    def _transform_position_matrix(self, position_matrix, transform_matrix):
        # position_matrix æ˜¯ H x W x 2 çš„ç›¸å¯¹ä½ç½®çŸ©é˜µ
        # transform_matrix æ˜¯ 4 x 4 çš„å˜æ¢çŸ©é˜µ
        
        H, W, _ = position_matrix.shape
        transformed_positions = torch.zeros_like(position_matrix)
        
        # å¯¹æ¯ä¸ªä½ç½®è¿›è¡Œå˜æ¢
        for i in range(H):
            for j in range(W):
                # åŸå§‹ç›¸å¯¹ä½ç½®åæ ‡ (x, y)
                x, y = position_matrix[i, j]
                
                # å°† (x, y) è½¬æ¢ä¸ºé½æ¬¡åæ ‡ (x, y, 0, 1)
                original_coords = torch.tensor([x, y, 0, 1.0])  # é½æ¬¡åæ ‡
                
                # åº”ç”¨å˜æ¢çŸ©é˜µ
                transformed_coords = torch.matmul(transform_matrix, original_coords.float())
                
                # è·å–å˜æ¢åçš„åæ ‡ï¼ˆä¸å†éœ€è¦é½æ¬¡åæ ‡ï¼‰
                transformed_positions[i, j] = transformed_coords[:2]  # åªå– x, y
        
        return transformed_positions


    def _create_ref_windows(self, tensor_list):
        device = tensor_list[0].device

        ref_windows = []
        for tensor in tensor_list:
            B, _, H, W = tensor.shape
            ref_y, ref_x = torch.meshgrid(
                torch.linspace(0.5, H - 0.5, H, dtype=torch.float32, device=device),
                torch.linspace(0.5, W - 0.5, W, dtype=torch.float32, device=device),
                indexing="ij",
            ) # ä¸¤ä¸ªshape éƒ½æ˜¯(H,W)

            ref_y = ref_y.reshape(-1)[None] / H
            ref_x = ref_x.reshape(-1)[None] / W
            ref_xy = torch.stack((ref_x, ref_y), -1) # (HW,2)
            ref_wh = torch.ones_like(ref_xy) * 0.025  # 0.01 - 0.05 w.r.t. Deform-DETR
            placeholder = torch.zeros_like(ref_xy)[..., :1]
            ref_box = torch.cat((ref_xy, placeholder + 0.5, ref_wh, placeholder + 0.5, placeholder), -1).expand(
                B, -1, -1
            )

            ref_windows.append(ref_box)
        ref_windows = torch.cat(ref_windows, dim=1)

        return ref_windows

    def _get_enc_proposals(self, enc_embed, ref_windows, indexes=None):
        '''
        è¿™ä¸ªå‡½æ•°ä¸»è¦æ˜¯è´Ÿè´£ç­›é€‰å‡ºåˆé€‚çš„query, ä¹Ÿå°±æ˜¯queryé€‰æ‹©. åŒæ—¶è¦å°†ä½ç½®ç¼–ç è¿›è¡Œæ—‹è½¬ï¼Œ ç»Ÿä¸€åˆ°egoçš„ä½ç½®ç¼–ç 
        '''
        B, L = enc_embed.shape[:2]
        out_logits, out_ref_windows = self.proposal_head(enc_embed, ref_windows) # ç”Ÿæˆproposal åˆ†ä¸º åˆ†ç±»logits(B, H * W, 1)  ã€boxes å®šä½ (B, H * W, 7)

        out_probs = out_logits[..., 0].sigmoid()
        topk_probs, indexes = torch.topk(out_probs, self.num_queries, dim=1, sorted=False) # é€‰å‡ºç½®ä¿¡åº¦è¾ƒé«˜çš„
        topk_probs = topk_probs.unsqueeze(-1)
        indexes = indexes.unsqueeze(-1)

        out_ref_windows = torch.gather(out_ref_windows, 1, indexes.expand(-1, -1, out_ref_windows.shape[-1]))
        out_ref_windows = torch.cat(
            (
                out_ref_windows.detach(),
                topk_probs.detach().expand(-1, -1, out_logits.shape[-1]),
            ),
            dim=-1,
        )

        out_pos = None
        out_embed = None

        return out_embed, out_pos, out_ref_windows, indexes

    @torch.no_grad()
    def _momentum_update_gt_decoder(self):
        """
        Momentum update of the key encoder
        """
        for param_q, param_k in zip(self.decoder.parameters(), self.decoder_gt.parameters()):
            param_k.data = param_k.data * self.m + param_q.data * (1.0 - self.m)

    def forward(self, src, pos, noised_gt_box=None, noised_gt_onehot=None, attn_mask=None, targets=None):
        assert pos is not None, "position encoding is required!"
        src_anchors = self._create_ref_windows(src)
        src, _, src_shape = flatten_with_shape(src, None)
        src_pos = []
        for pe in pos:
            B, C = pe.shape[:2]
            pe = pe.view(B, C, -1).transpose(1, 2) # (B, HW, C)
            src_pos.append(pe)
        src_pos = torch.cat(src_pos, dim=1) # (B, H*W, C)
        src_start_index = torch.cat([src_shape.new_zeros(1), src_shape.prod(1).cumsum(0)[:-1]]) # è¿™ä¸ªæ˜¯ç”¨äºä¸€æ¬¡å¤„ç†å¤šä¸ªå°ºåº¦çš„featureçš„ï¼Œåœ¨æˆ‘ä»¬è¿™é‡Œå°±æ˜¯(0,)

        memory = self.encoder(src, src_pos, src_shape, src_start_index, src_anchors) # (B, H*W, 256) é€šè¿‡BoxAttentionè¿›è¡Œäº†äº¤äº’
        query_embed, query_pos, topk_proposals, topk_indexes = self._get_enc_proposals(memory, src_anchors)# è¿”å›Noneï¼ŒNoneï¼Œ(B, query_num, 8)ï¼Œ(B, query_num, 1)

        select_memory = torch.gather(memory, 1, topk_indexes.expand(-1, -1, memory.shape[-1])) # (B, query_num, 256)

        return select_memory, topk_proposals, None, memory, src_anchors, topk_indexes
        if noised_gt_box is not None:
            noised_gt_proposals = torch.cat(
                (
                    noised_gt_box,
                    noised_gt_onehot,
                ),
                dim=-1,
            )
            topk_proposals = torch.cat(
                (
                    noised_gt_proposals,
                    topk_proposals,
                ),
                dim=1,
            )
        init_reference_out = topk_proposals[..., :7]

        # hs, inter_references = self.decoder_gt(
        hs, inter_references = self.decoder(
            query_embed,
            query_pos,
            memory,
            src_shape,
            src_start_index,
            topk_proposals,
            attn_mask,
        )

        # optional gt forward
        if targets is not None:
            batch_size = len(targets)
            per_gt_num = [tgt["gt_boxes"].shape[0] for tgt in targets]
            max_gt_num = max(per_gt_num)
            batched_gt_boxes_with_score = memory.new_zeros(batch_size, max_gt_num, 8)
            for bi in range(batch_size):
                batched_gt_boxes_with_score[bi, : per_gt_num[bi], :7] = targets[bi]["gt_boxes"]
                batched_gt_boxes_with_score[bi, : per_gt_num[bi], 7:] = F.one_hot(
                    targets[bi]["labels"], num_classes=self.num_classes
                )

            with torch.no_grad():
                self._momentum_update_gt_decoder()
                if noised_gt_box is not None:
                    dn_group_num = noised_gt_proposals.shape[1] // (max_gt_num * 2)
                    pos_idxs = list(range(0, dn_group_num * 2, 2))
                    pos_noised_gt_proposals = torch.cat(
                        [noised_gt_proposals[:, pi * max_gt_num : (pi + 1) * max_gt_num] for pi in pos_idxs],
                        dim=1,
                    )
                    gt_proposals = torch.cat((batched_gt_boxes_with_score, pos_noised_gt_proposals), dim=1)
                    # create attn_mask for gt groups
                    gt_attn_mask = memory.new_ones(
                        (dn_group_num + 1) * max_gt_num, (dn_group_num + 1) * max_gt_num
                    ).bool()
                    for di in range(dn_group_num + 1):
                        gt_attn_mask[
                            di * max_gt_num : (di + 1) * max_gt_num,
                            di * max_gt_num : (di + 1) * max_gt_num,
                        ] = False
                else:
                    gt_proposals = batched_gt_boxes_with_score
                    gt_attn_mask = None

                hs_gt, inter_references_gt = self.decoder_gt(
                    None,
                    None,
                    memory,
                    src_shape,
                    src_start_index,
                    gt_proposals,
                    gt_attn_mask,
                )

            init_reference_out = torch.cat(
                (
                    init_reference_out,
                    gt_proposals[..., :7],
                ),
                dim=1,
            )

            hs = torch.cat(
                (
                    hs,
                    hs_gt,
                ),
                dim=2,
            )
            inter_references = torch.cat(
                (
                    inter_references,
                    inter_references_gt,
                ),
                dim=2,
            )

        inter_references_out = inter_references
        return hs, init_reference_out, inter_references_out, memory, src_anchors, topk_indexes


# simple fusion use Scaled Dot Product Attention
class AttenQueryFusion(nn.Module):
    def __init__(self, feature_dim):
        super(AttenQueryFusion, self).__init__()
        self.sqrt_dim = np.sqrt(feature_dim)

    def forward(self, x):
        # x : 1, k, C
        if x.size(1) == 1:
            return x
        query = key = value = x
        score = torch.bmm(query, key.transpose(1, 2)) / self.sqrt_dim
        attn = F.softmax(score, -1)
        context = torch.bmm(attn, value) # (1,k,C)
        x = context[:,0:1,:]
        return x

class SimpleGatingFusion(nn.Module):
    """
    å¯¹ç‰¹å¾è¿›è¡Œèåˆçš„æ¨¡å—:
    è¾“å…¥:
        x: (1, k, C) åŒä¸€ä½ç½®kä¸ªAgentç‰¹å¾, k<=4
    å®ç°:
        1. Flattenæˆä¸º (1, k*C)
        2. é€šè¿‡MLPè¾“å‡º(k,)ç»´æƒé‡å‘é‡
        3. ä½¿ç”¨softmaxå½’ä¸€åŒ–ååŠ æƒæ±‚å’Œ
    """
    def __init__(self, d_model=256, max_agents=2, hidden_dim=128):
        super(SimpleGatingFusion, self).__init__()
        self.max_agents = max_agents
        self.d_model = d_model
        self.mlp = nn.Sequential(
            nn.Linear(d_model * max_agents, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, max_agents)
        )

    def forward(self, x):
        # x: (1, k, C), k <= self.max_agents
        k = x.size(1)
        if k == 0:
            # æ²¡æœ‰ç‰¹å¾å¯èåˆï¼Œç›´æ¥è¿”å›ç©º
            return x
        if k == 1:
            # åªæœ‰ä¸€ä¸ªç‰¹å¾ï¼Œç›´æ¥è¿”å›
            return x

        # å¦‚æœå®é™…k < max_agentsï¼Œç”¨0å¡«å……
        pad_num = self.max_agents - k
        if pad_num > 0:
            pad = x.new_zeros((1, pad_num, self.d_model))
            x_padded = torch.cat([x, pad], dim=1) # (1, max_agents, C)
        else:
            x_padded = x

        flattened = x_padded.view(1, -1)  # (1, max_agents*C)
        weights = self.mlp(flattened)     # (1, max_agents)
        weights = weights[:, :k]          # åªå–å‰kä¸ªæƒé‡
        weights = F.softmax(weights, dim=-1)  # (1, k)
        
        # åŠ æƒæ±‚å’Œ
        fused = torch.sum(x * weights.unsqueeze(-1), dim=1, keepdim=True) # (1,1,C)
        return fused

class BoxGatingFusion(nn.Module):
    """
    å¯¹å‚è€ƒæ¡†è¿›è¡Œèåˆçš„æ¨¡å—:
    è¾“å…¥:
        boxes: (1, k, 8) åŒä¸€ä½ç½®kä¸ªAgentæä¾›çš„boxå‚æ•°, k<=4
    å®ç°:
        ä¸ç‰¹å¾ç±»ä¼¼ï¼Œç”¨MLPå¯¹kä¸ªboxæ‰“åˆ†å¹¶åŠ æƒå¹³å‡ã€‚
    å‡è®¾8ç»´boxä¸º [cx, cy, cz, w, l, h, rot, score]
    """
    def __init__(self, box_dim=8, max_agents=2, hidden_dim=64):
        super(BoxGatingFusion, self).__init__()
        self.max_agents = max_agents
        self.box_dim = box_dim
        self.mlp = nn.Sequential(
            nn.Linear(box_dim * max_agents, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, max_agents)
        )

    def forward(self, boxes):
        # boxes: (1, k, 8)
        k = boxes.size(1)
        if k == 0:
            return boxes
        if k == 1:
            return boxes

        pad_num = self.max_agents - k
        if pad_num > 0:
            pad = boxes.new_zeros((1, pad_num, self.box_dim))
            boxes_padded = torch.cat([boxes, pad], dim=1) # (1, max_agents, 8)
        else:
            boxes_padded = boxes

        flattened = boxes_padded.view(1, -1) # (1, max_agents*8)
        weights = self.mlp(flattened) # (1, max_agents)
        weights = weights[:, :k] 
        weights = F.softmax(weights, dim=-1) # (1, k)

        # å–ç©ºé—´å‚æ•°7ç»´åŠ æƒå¹³å‡ï¼Œå†å¯¹scoreç»´åº¦ä¹ŸåŠ æƒå¹³å‡
        space_params = boxes[..., :7] # (1,k,7)
        fused_space = torch.sum(space_params * weights.unsqueeze(-1), dim=1, keepdim=True) # (1,1,7)

        score = boxes[..., 7:8] # (1,k,1)
        fused_score = torch.sum(score * weights.unsqueeze(-1), dim=1, keepdim=True) # (1,1,1)

        fused_box = torch.cat([fused_space, fused_score], dim=-1) # (1,1,8)
        return fused_box


class Transformer(nn.Module):
    def __init__(
        self,
        d_model=256,
        nhead=8,
        nlevel=1,
        num_encoder_layers=6,
        num_decoder_layers=6,
        dim_feedforward=1024,
        dropout=0.1,
        activation="relu",
        num_queries=300,
        num_classes=1,
        mom=0.999,
        cp_flag=False,
    ):
        super().__init__()

        self.num_queries = num_queries
        self.num_classes = num_classes
        self.m = mom
        self.extra_query_num = 200 # é¢å¤–çš„queryæ•°é‡ï¼Œç”¨äºéé‡å ä½ç½®çš„è¡¥å……

        encoder_layer = TransformerEncoderLayer(d_model, nhead, nlevel, dim_feedforward, dropout, activation)
        self.encoder = TransformerEncoder(d_model, encoder_layer, num_encoder_layers)
        self.trans_adapter = TransAdapt(d_model, nhead, nlevel, dim_feedforward, dropout, activation)
        # self.query_fusion = AttenQueryFusion(d_model)
        # self.ref_fusion = AttenQueryFusion(8)
        self.query_fusion = SimpleGatingFusion()
        self.ref_fusion = BoxGatingFusion()
        self.debug = 0
        decoder_layer = TransformerDecoderLayer(d_model, nhead, nlevel, dim_feedforward, dropout, activation)
        self.decoder = TransformerDecoder(d_model, decoder_layer, num_decoder_layers, cp_flag)

    def _create_ref_windows(self, tensor_list):
        device = tensor_list[0].device

        ref_windows = []
        for tensor in tensor_list:
            B, _, H, W = tensor.shape
            ref_y, ref_x = torch.meshgrid(
                torch.linspace(0.5, H - 0.5, H, dtype=torch.float32, device=device),
                torch.linspace(0.5, W - 0.5, W, dtype=torch.float32, device=device),
                indexing="ij",
            )

            ref_y = ref_y.reshape(-1)[None] / H
            ref_x = ref_x.reshape(-1)[None] / W
            ref_xy = torch.stack((ref_x, ref_y), -1)
            ref_wh = torch.ones_like(ref_xy) * 0.025  # 0.01 - 0.05 w.r.t. Deform-DETR
            placeholder = torch.zeros_like(ref_xy)[..., :1]
            ref_box = torch.cat((ref_xy, placeholder + 0.5, ref_wh, placeholder + 0.5, placeholder), -1).expand(
                B, -1, -1
            )

            ref_windows.append(ref_box)
        ref_windows = torch.cat(ref_windows, dim=1)

        return ref_windows

    def _get_enc_proposals(self, enc_embed, ref_windows, indexes=None):
        B, L = enc_embed.shape[:2]
        out_logits, out_ref_windows = self.proposal_head(enc_embed, ref_windows)

        out_probs = out_logits[..., 0].sigmoid()
        topk_probs, indexes = torch.topk(out_probs, self.num_queries + self.extra_query_num, dim=1, sorted=True)
        topk_probs = topk_probs.unsqueeze(-1)
        indexes = indexes.unsqueeze(-1)
        # print("out_probs  is ", [round(x, 3) for x in out_probs[0][:1000].tolist()])

        out_ref_windows = torch.gather(out_ref_windows, 1, indexes.expand(-1, -1, out_ref_windows.shape[-1]))
        out_ref_windows = torch.cat(
            (
                out_ref_windows.detach(),
                topk_probs.detach().expand(-1, -1, out_logits.shape[-1]),
            ),
            dim=-1,
        )

        out_pos = None
        out_embed = None

        return out_embed, out_pos, out_ref_windows, indexes

    @torch.no_grad()
    def _momentum_update_gt_decoder(self):
        """
        Momentum update of the key encoder
        """
        for param_q, param_k in zip(self.decoder.parameters(), self.decoder_gt.parameters()):
            param_k.data = param_k.data * self.m + param_q.data * (1.0 - self.m)

    def regroup(self, x, record_len):
        cum_sum_len = torch.cumsum(record_len, dim=0)
        split_x = torch.tensor_split(x, cum_sum_len[:-1].cpu())
        return split_x

    def forward(self, src, pos, noised_gt_box=None, noised_gt_onehot=None, attn_mask=None, targets=None, record_len=None, pairwise_t_matrix=None, pairwise_t_matrix_ref=None, box_encode_func=None, box_decode_func=None):
        '''
        src: [(B_n, 256, H, W)]
        pos: [(B_n, 256, H, W)]
        noised_gt_box: (B, pad_size, 7)  è¿™é‡Œç”¨çš„åº”è¯¥æ˜¯ååŒgt
        noised_gt_onehot: (B, pad_size, num_classes)
        attn_mask: (1000+pad_size, 1000+pad_size)
        targets: [{'gt_boxes': (N, 7), 'labels': (N, )}, ...]
        '''
        assert pos is not None, "position encoding is required!"
        src_anchors = self._create_ref_windows(src) # åˆ›é€ å‚è€ƒæ¡†ï¼Œè¿™ä¸ªæ˜¯BoxAttentionå¿…é¡»çš„ (B_n, HW, 7)
        src, _, src_shape = flatten_with_shape(src, None)# å±•å¹³ç‰¹å¾å›¾ï¼Œè¿”å›çš„æ˜¯ (B_n, H*W, 256), None, (1, 2) æœ€åä¸€é¡¹è®°å½•ç€Hï¼ŒW å³feature shape
        src_pos = []
        for pe in pos:
            B, C = pe.shape[:2]
            pe = pe.view(B, C, -1).transpose(1, 2) # b, h*w, c
            src_pos.append(pe)
        src_pos = torch.cat(src_pos, dim=1) # (B_n, H*W, C)
        src_start_index = torch.cat([src_shape.new_zeros(1), src_shape.prod(1).cumsum(0)[:-1]]) # è¿™æ˜¯ä¸ºäº†ç”Ÿæˆåˆ’åˆ†çš„ç´¢å¼•ï¼ŒåŒºåˆ†æ¯ä¸ªç‰¹å¾å›¾çš„ä½ç½®ï¼Œç”±äºåªæœ‰ä¸€ä¸ªç‰¹å¾å›¾ï¼Œæ‰€ä»¥ç»“æœæ˜¯(0,)

        memory = self.encoder(src, src_pos, src_shape, src_start_index, src_anchors) # BoxAttention æå–ç‰¹å¾ ç»“æœä¸º(B_n, H*W, 256)
        query_embed, query_pos, topk_proposals, topk_indexes = self._get_enc_proposals(memory, src_anchors) # è¿”å›Noneï¼ŒNoneï¼Œ(B_n, query_num+extra_num, 8)ï¼Œ(B_n, query_num+extra_num, 1)
        # print("topk_indexes first is ", topk_indexes[0].tolist())
        # print("memory shape is ", memory.shape)
        ego_topk_proposals = topk_proposals[:, :self.num_queries, :] # (B_n, query_num, 8)
        ego_topk_indexes = topk_indexes[:, :self.num_queries, :] # (B_n, query_num, 1)
        extra_topk_proposals = topk_proposals[:, self.num_queries:, :]  # (B_n, extra_num, 8)
        extra_topk_indexes = topk_indexes[:, self.num_queries:, :]  # (B_n, extra_num, 1)

        fined_query = torch.gather(memory, 1, ego_topk_indexes.expand(-1, -1, memory.shape[-1])) # (B_n, query_num, C) refineçš„query
        extra_query = torch.gather(memory, 1, extra_topk_indexes.expand(-1, -1, memory.shape[-1])) # (B_n, extra_num, C) refineçš„query

        H, W = src_shape[0,0], src_shape[0,1]
        memory_discrete = torch.zeros_like(memory) # (B_n, H*W, 256) 
        memory_discrete = memory_discrete.scatter(1, ego_topk_indexes.repeat(1, 1, memory_discrete.size(-1)), fined_query) # (B_n, H*W, 256) å°†queryæ”¾å…¥åˆ°ä¸€ä¸ªç©ºçš„memoryä¸­
        memory_discrete = memory_discrete.permute(0, 2, 1).reshape(memory.shape[0], memory.shape[-1], H, W) # (B_n, C, H, W) å½¢æˆç¨€ç–çš„ç‰¹å¾å›¾

        # æ–°å»ºä¸€ä¸ªé»˜è®¤å‚è€ƒæ¡†ï¼Œç„¶åå°†encoderé¢„æµ‹çš„å†…å®¹å¡«å……è¿›å»ï¼Œè¿™ä¸ªå°†ä¼šåœ¨ç©ºé—´å˜æ¢åä½œä¸º
        ref_boxes_before_trans = copy.deepcopy(src_anchors)
        ref_probs_before_trans = torch.zeros(ref_boxes_before_trans.size(0), ref_boxes_before_trans.size(1), 1).to(ref_boxes_before_trans)
        ref_boxes_before_trans = torch.cat([ref_boxes_before_trans, ref_probs_before_trans], dim=-1)
        fined_ref_boxes = ego_topk_proposals # (B_n, query_num, 8) è¿™ä¸ªæ˜¯å‚è€ƒæ¡† è¦è·Ÿç€é‡‡æ ·
        ref_boxes_before_trans = ref_boxes_before_trans.scatter(1, ego_topk_indexes.repeat(1, 1, ref_boxes_before_trans.size(-1)), fined_ref_boxes) # (B_n, H*W, 8) å°†queryæ”¾å…¥åˆ°ä¸€ä¸ªç©ºçš„memoryä¸­

        ref_boxes_before_trans = ref_boxes_before_trans.permute(0, 2, 1).reshape(memory.shape[0], 8, H, W) # (B_n, 8, H, W) å½¢æˆç¨€ç–çš„ç‰¹å¾å›¾

        # åˆ›é€ maskæ ‡è®°fined query
        valid_flag = torch.ones(fined_query.shape[0], fined_query.shape[1], 1).to(fined_query) # (B_n, query_num, 1) å…¨1
        memory_mask = torch.zeros(memory.shape[0], memory.shape[1], 1).to(memory) # (B_n, HW, 1)
        memory_mask = memory_mask.scatter(1, ego_topk_indexes.repeat(1, 1, memory_mask.size(-1)), valid_flag) # (B_n, HW, 1)  å°†fined queryç»™æ ‡è®°
        memory_mask = memory_mask.permute(0, 2, 1).reshape(memory_mask.shape[0], 1, H, W) # (B_n, 1, H, W)

        memory_batch_lst = self.regroup(memory, record_len)
        memory_discrete_batch_lst = self.regroup(memory_discrete, record_len)
        ref_boxes_before_trans_batch_lst = self.regroup(ref_boxes_before_trans, record_len)
        memory_mask_batch_lst = self.regroup(memory_mask, record_len)

        ego_topk_indexes_batch_lst = self.regroup(ego_topk_indexes, record_len)
        extra_topk_indexes_batch_lst = self.regroup(extra_topk_indexes, record_len)
        extra_query_batch_lst = self.regroup(extra_query, record_len)
        extra_topk_proposals_batch_lst = self.regroup(extra_topk_proposals, record_len) #  [(N1, extra_num, 8), (N2, extra_num, 8)...]

        fused_queries = []
        fused_ref_windows = []
        fused_indicies = []
        ego_features = []
        # å°†å…¶ä»–çš„agentçš„feature æŠ•å½±åˆ°egoåæ ‡ç³»
        for bid in range(len(memory_batch_lst)):
            N = record_len[bid] # number of valid agent
            
            memory_b = memory_batch_lst[bid] # (N, H*W, C) å•ç‹¬ä¸€ä¸ªæ ·æœ¬ä¸‹çš„Nä¸ªagentï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªä¸ºegoçš„feature
            memory_discrete_b = memory_discrete_batch_lst[bid] # (N, C, H, W) Encoderç­›é€‰è¿‡çš„ç•™ä¸‹æ¥ï¼Œå…¶ä½™å…¨éƒ¨ä¸ºç©º
            ref_boxes_trans_b = ref_boxes_before_trans_batch_lst[bid][:,:7,:,:] # (N, 7, H, W) Encoderç­›é€‰è¿‡çš„ç•™ä¸‹æ¥ï¼Œå…¶ä½™å…¨éƒ¨ä¸ºç©º
            ref_probs_trans_b = ref_boxes_before_trans_batch_lst[bid][:,7:,:,:] # (N, 1, H, W) Encoderç­›é€‰è¿‡çš„ç•™ä¸‹æ¥ï¼Œå…¶ä½™å…¨éƒ¨ä¸ºç©º
            memory_mask_b = memory_mask_batch_lst[bid] # (N, 1, H, W)
            t_matrix = pairwise_t_matrix[bid][:N, :N, :, :] # (N, N, 2, 3)
            t_matrix_ref = pairwise_t_matrix_ref[bid][:N, :N, :, :] # (N, N, 4, 4)
            # print("bid is ", bid)
            # print("record_len is ", record_len)
            # print("memory_discrete_b shape is ", memory_discrete_b.shape)
            # print("t_matrix shape is ", t_matrix.shape)
            neighbor_memory = warp_affine_simple(memory_discrete_b, t_matrix[0, :, :, :], (H, W), mode='nearest') # (N, C, H, W)
            ref_boxes_trans_b = warp_affine_simple(ref_boxes_trans_b, t_matrix[0, :, :, :], (H, W), mode='nearest') # (N, 7, H, W)
            neighbor_memory_mask = warp_affine_simple(memory_mask_b, t_matrix[0, :, :, :], (H, W), mode='nearest') # (N, 1, H, W)

            ref_boxes_trans_b = torch.cat([ref_boxes_trans_b, ref_probs_trans_b], dim=1) # (N, 8, H, W)
            neighbor_memory = neighbor_memory.flatten(2).permute(0, 2, 1) # (N, HW, C)
            ref_boxes_trans_b = ref_boxes_trans_b.flatten(2).permute(0, 2, 1) # (N, HW, 8)
            neighbor_memory_mask = neighbor_memory_mask.flatten(2).permute(0, 2, 1) # (N, HW, 1) è¿™ä¸ªé‡Œé¢æœ‰0æœ‰1, 1çš„åœ°æ–¹å°±æ˜¯å¯¹åº”å…¶æœ‰æ•ˆçš„queryï¼Œè¿™äº›queryè¦å…ˆåœ¨ego featureä¸ŠåšLocal Attention
            # pos_b = src_pos[0:N] # (N, HW, C) NOTE ä½ç½®ç¼–ç æ¯ä¸ªfeatureåœ¨ä¸€å¼€å§‹æ˜¯å®Œå…¨ä¸€æ ·çš„ æ‰€ä»¥å¯ä»¥ç›´æ¥å–éœ€è¦çš„ä¸ªæ•°

            neighbor_mask = neighbor_memory_mask.squeeze(-1).bool() # (N, HW)
            valid_features_lst = [neighbor_memory[i][neighbor_mask[i]].unsqueeze(0) for i in range(N)] # [(1, n1, C), (1, n2, C)...]
            valid_ref_lst = [ref_boxes_trans_b[i][neighbor_mask[i]].unsqueeze(0) for i in range(N)] # [(1, n1, 8), (1, n2, 8)...]
            record_query_num = torch.tensor([v.size(1) for v in valid_ref_lst]) # [n1, n2, ...]
            # valid_pos_lst = [pos_b[i][neighbor_mask[i]] for i in range(N)] # [(n1, C), (n2, C)...]

            none_ego_features_lst = valid_features_lst[1:] # [(1, n2, C), ...]
            none_ego_ref = valid_ref_lst[1:] # [(1, n2, 8), ...]
            # none_ego_pos = valid_pos_lst[1:]

            none_ego_ref_trans_lst = []
            # æ—‹è½¬å‚è€ƒæ¡†ï¼Œæš‚æ—¶æ²¡æç©ºé—´å˜æ¢çŸ©é˜µçš„ç¼©æ”¾ï¼Œå¦‚æœç›´æ¥ç¼©æ”¾ç©ºé—´å˜æ¢çŸ©é˜µåˆ™ä¸ç”¨encodeå’Œdecode boxï¼Œä½†æ˜¯ç›®å‰å…ˆä»¥è¿™æ ·çš„æ–¹å¼éªŒè¯é€»è¾‘ TODO åé¢è¦æ”¹
            for id, nef in enumerate(none_ego_ref):
                none_ego_bbox_center = box_decode_func(nef[..., :7].squeeze(0)) # (n, 7) åå½’ä¸€åŒ–

                none_ego_bbox_corner = box_utils.boxes_to_corners_3d(none_ego_bbox_center, 'lwh') # (n, 8, 3)
                projected_none_ego_bbox_corner = box_utils.project_box3d(none_ego_bbox_corner.float(), t_matrix_ref[0,id+1].float())
                projected_none_ego_bbox_center = box_utils.corners_to_boxes_3d(projected_none_ego_bbox_corner, 'lwh') # (n, 7)
                # print("id is ", id)
                # print("t_matrix_ref is ", t_matrix_ref[0,id+1])
                # print("none_ego_bbox_center is ", none_ego_bbox_center)
                # print("projected_none_ego_bbox_center is ", projected_none_ego_bbox_center)
                # xxx
                projected_none_ego_bbox_center = box_encode_func(projected_none_ego_bbox_center) # é‡æ–°å½’ä¸€åŒ–
                projected_none_ego_bbox_center = torch.cat([projected_none_ego_bbox_center, nef[0, :, 7:]], dim=-1) # # (n, 8)
                none_ego_ref_trans_lst.append(projected_none_ego_bbox_center.unsqueeze(0))

                # è¿˜è¦å°†å˜æ¢åçš„æ”¾å…¥åˆ° valid_ref_lst
                valid_ref_lst[id+1] = none_ego_ref_trans_lst[-1]

            if len(none_ego_features_lst) > 0:
                none_ego_features = torch.cat(none_ego_features_lst, dim=1) # (1, n2+n3+..., C)
                none_ego_ref_trans = torch.cat(none_ego_ref_trans_lst, dim=1) # (1, n2+n3+..., 8)
                # none_ego_pos = torch.cat(none_ego_pos, dim=0) # (n2+n3+..., C) # XXX è€ƒè™‘ä¸€ä¸‹posæ˜¯ä½¿ç”¨refè¿˜æ˜¯ç”¨egoçš„ä½ç½®ç¼–ç ï¼Œ ç›®å‰ä½¿ç”¨refä½œä¸ºposç¼–ç  æ‰€ä»¥è¿™ä¸ªæš‚æ—¶ä¸éœ€è¦
            
                # TODO è¿™é‡Œä»…ä»…å¯¹queryåšäº† Local Attentionï¼Œä½†å¹¶æ²¡æœ‰æ®æ­¤å»æ›´æ–°æ—‹è½¬è¿‡æ¥çš„å‚è€ƒæ¡† æ„Ÿè§‰æ˜¯éœ€è¦æ›´æ–°çš„ 
                query_adapt = self.trans_adapter(none_ego_features, memory_b[0:1], src_shape, src_start_index, none_ego_ref_trans) # (1, n2+n3+..., C) å…¶ä»–agentçš„queryåœ¨ego featureä¸Šè¿›è¡ŒLocal Attention

                query_adapt_lst = self.regroup(query_adapt.squeeze(0), record_query_num[1:]) # [(n2, C), ...]

                query_lst = [q.unsqueeze(0) for q in query_adapt_lst]  # [(1, n2, C), ...]
            else: # å¯èƒ½çš„æƒ…å†µ: 1. è·ç¦»åŸå› å¯¼è‡´åªæœ‰egoä¸€ä¸ªfeature 2. agentæŠ•å½±è¿‡æ¥æ— query
                query_lst = []

            query_lst = valid_features_lst[0:1] + query_lst  # [(1, n1, C), (1, n2, C)...]

            all_indices = [] # [(1, n1, 1), (1, n2, 1), (1, n3, 1)...] ä¸€å…±N-1 ä¸ª, è¡¨ç¤ºåœºæ™¯ä¸­çš„æ‰€æœ‰æœ‰æ•ˆqueryçš„ç´¢å¼• å…¶ä¸­egoæˆ‘ä»¬ä¸ç”¨
            for i in range(N):
                neighbor_index = torch.nonzero(neighbor_memory_mask[i].squeeze(-1), as_tuple=False) # (n, 1)
                if neighbor_index.size(0) > 0:
                    all_indices.append(neighbor_index.unsqueeze(0))
            all_indices[0] = ego_topk_indexes_batch_lst[bid][0:1] # (N, query_num, 1)ä¸­é€‰æ‹©å‡ºegoçš„ å³(1, query_num, 1)

            ego_feature = memory_b[0:1] # (1, HW, C)

            # æ¥ä¸‹æ¥å¯¹ç›¸åŒä½ç½®çš„queryè¿›è¡Œèåˆï¼Œagentæä¾›çš„é¢å¤–ä¿¡æ¯åˆ™æ”¾ç½®åœ¨extraçš„ä½ç½®
            if len(all_indices) > 1:
                fused_query, fused_indices = self.fuse_features_by_index(all_indices, query_lst, self.query_fusion, extra_query_batch_lst[bid][0:1], extra_topk_indexes_batch_lst[bid][0:1]) # (1, 300+200, C), (1, 300+200, 1)
                fused_ref, _ = self.fuse_features_by_index(all_indices, valid_ref_lst, self.ref_fusion, extra_topk_proposals_batch_lst[bid][0:1], extra_topk_indexes_batch_lst[bid][0:1]) # (1, 300+200, 8)
                ego_feature = ego_feature.scatter(1, fused_indices.repeat(1, 1, ego_feature.size(-1)), fused_query)
            else: # å¦‚æœåˆ°è¿™é‡Œï¼Œå¯èƒ½æ˜¯: 1.è·ç¦»è¿‡è¿œå¯¼è‡´åªæœ‰ä¸€ä¸ªego 2.agentæŠ•å½±è¿‡æ¥æ— query
                fused_query = torch.cat([query_lst[0], extra_query_batch_lst[bid][0:1]], dim=1)
                fused_indices = torch.cat([all_indices[0], extra_topk_indexes_batch_lst[bid][0:1]], dim=1)
                fused_ref = torch.cat([valid_ref_lst[0], extra_topk_proposals_batch_lst[bid][0:1]], dim=1)
                # print("crazy, without overlap! ")
                
            fused_queries.append(fused_query)
            fused_indicies.append(fused_indices)
            fused_ref_windows.append(fused_ref)
            ego_features.append(ego_feature)
        fused_queries = torch.cat(fused_queries, dim=0) # (B, query_num+extra_num, C)
        fused_indicies = torch.cat(fused_indicies, dim=0) # (B, query_num+extra_num, 1)
        fused_ref_windows = torch.cat(fused_ref_windows, dim=0) # (B, query_num+extra_num, 8)
        ego_features = torch.cat(ego_features, dim=0) # (B, HW, C)
        # print("fused_indicies first is ", fused_indicies[0].tolist())
        # print("ego_topk_indexes is ", ego_topk_indexes[0].tolist())
        # print("record_query_num is ", record_query_num)
        # if self.debug < 100:
        #     self.debug += 1
        # else:
        #     xxx
        # åŠ å™ªå£°gtï¼Œå‡†å¤‡ä¸€èµ·å‚ä¸decoderè®­ç»ƒå»å™ª
        if noised_gt_box is not None:
            noised_gt_proposals = torch.cat(
                (
                    noised_gt_box,
                    noised_gt_onehot,
                ),
                dim=-1,
            ) # (B, pad_size, 8)
            fused_ref_windows = torch.cat(
                (
                    noised_gt_proposals,
                    fused_ref_windows,
                ),
                dim=1,
            ) # (B, pad_size + all_query_num, 8) while: all_query_num == query_num+extra_num
        init_reference_out = fused_ref_windows[..., :7]

        # hs, inter_references = self.decoder_gt(
        hs, inter_references = self.decoder(
            query_embed, # None
            query_pos, # None
            ego_features, # BoxAttention æå–ç‰¹å¾åç»“åˆå¤šagentåçš„Feature Map ç»“æœä¸º(B, H*W, 256)
            src_shape, # (1, 2)
            src_start_index, # (0,)
            fused_ref_windows, # (B, all_query_num, 8)
            attn_mask,
        ) # (3, B, pad_size + all_query_num, 256) æ¯ä¸€å±‚çš„è¾“å‡ºçš„queryç‰¹å¾ï¼Œ (3ï¼Œ B, pad_size + all_query_num, 7) æ¯ä¸€å±‚çš„æ£€æµ‹ç»“æœ

        # optional gt forward å¯¹æ¯”å­¦ä¹ éœ€è¦ç”¨åˆ°çš„åŠ¨é‡æ›´æ–°æ¨¡å‹ç”¨åŠ å™ªgtæ¥åšå¯¹æ¯”å­¦ä¹ çš„
        if targets is not None:
            batch_size = len(targets) # è¿™é‡Œæ˜¯ååŒæ ‡ç­¾
            per_gt_num = [tgt["gt_boxes"].shape[0] for tgt in targets] # [N1, N2, N3, N4] æ­¤ä¸ºB=4æ—¶çš„å„ä¸ªæ ·æœ¬çš„GTæ•°
            max_gt_num = max(per_gt_num)
            batched_gt_boxes_with_score = memory.new_zeros(batch_size, max_gt_num, 8) # (B, max_gt_num, 8)
            for bi in range(batch_size):
                batched_gt_boxes_with_score[bi, : per_gt_num[bi], :7] = targets[bi]["gt_boxes"] # æ”¾å…¥gtçš„box å’Œ one-hot åˆ†ç±»ç¼–ç 
                batched_gt_boxes_with_score[bi, : per_gt_num[bi], 7:] = F.one_hot(
                    targets[bi]["labels"], num_classes=self.num_classes
                )

            with torch.no_grad():
                self._momentum_update_gt_decoder() # åŠ¨é‡æ›´æ–°è¾…åŠ©æ¨¡å‹ï¼Œå…¶å‚æ•°æ›´æ–°é€Ÿåº¦éå¸¸ç¼“æ…¢ï¼Œä½†ä¸€ç›´è¿½éšdecoder
                if noised_gt_box is not None:
                    dn_group_num = noised_gt_proposals.shape[1] // (max_gt_num * 2) # å¾—åˆ°å»å™ªgtç»„æ•° == 3  2æŒ‡çš„æ˜¯æ¯ä¸€ç»„åˆåˆ†æ­£è´Ÿæ ·æœ¬
                    pos_idxs = list(range(0, dn_group_num * 2, 2))
                    pos_noised_gt_proposals = torch.cat(
                        [noised_gt_proposals[:, pi * max_gt_num : (pi + 1) * max_gt_num] for pi in pos_idxs],
                        dim=1,
                    ) # æ¯ä¸€ç»„æŠ½å–max_gt_numä¸ª (B, 3*max_gt_num, 8) è¿™æ˜¯ç›¸å½“äºå»å™ªæ­£æ ·æœ¬æŠ½å–å‡ºæ¥
                    gt_proposals = torch.cat((batched_gt_boxes_with_score, pos_noised_gt_proposals), dim=1)
                    # create attn_mask for gt groups
                    gt_attn_mask = memory.new_ones(
                        (dn_group_num + 1) * max_gt_num, (dn_group_num + 1) * max_gt_num
                    ).bool()  # ï¼ˆ4*max_gt_numï¼Œ4*max_gt_numï¼‰å…¨True
                    for di in range(dn_group_num + 1): # å¯¹è§’éƒ¨åˆ†mask å…¨éƒ¨è®¾ç½®ä¸ºFalseï¼Œç›¸å½“äºè¯´åªå…³æ³¨è‡ªå·±ï¼Œå³æ¯ä¸€æ‰¹gtï¼Œæ— è®ºæœ‰æ— å™ªå£°ï¼Œä»…å…³æ³¨è‡ªèº«ï¼Œå±è”½ç»„ä¹‹é—´çš„å¯è§æ€§
                        gt_attn_mask[
                            di * max_gt_num : (di + 1) * max_gt_num,
                            di * max_gt_num : (di + 1) * max_gt_num,
                        ] = False
                else:
                    gt_proposals = batched_gt_boxes_with_score
                    gt_attn_mask = None

                hs_gt, inter_references_gt = self.decoder_gt( # è¾…åŠ©æ¨¡å‹è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œç¼“æ…¢è¿½éšdecoderã€‚ è¿”å› (3ï¼ŒB, 4*max_gt_num, 256) ä¸ (3ï¼ŒB, 4*max_gt_num, 8)
                    None,
                    None,
                    ego_features, # BoxAttention æå–ç‰¹å¾åç»“åˆå¤šagentåçš„Feature Map ç»“æœä¸º(B, H*W, 256)
                    src_shape, # (1, 2)
                    src_start_index, # (0,)
                    gt_proposals, # (B, 4*max_gt_num, 8)
                    gt_attn_mask, #ï¼ˆ4*max_gt_numï¼Œ4*max_gt_numï¼‰
                )

            init_reference_out = torch.cat(
                (
                    init_reference_out,
                    gt_proposals[..., :7],
                ),
                dim=1,
            ) # (B, pad_size + all_query_num + 4*max_gt_num, 8) while: all_query_num == query_num+extra_num è¾“å…¥decoderå‰çš„ref window

            hs = torch.cat(
                (
                    hs,
                    hs_gt,
                ),
                dim=2,
            ) # (3, B, pad_size + all_query_num + 4*max_gt_num, 256) æ¯ä¸€å±‚Decoder layerçš„è¾“å‡ºquery
            inter_references = torch.cat(
                (
                    inter_references,
                    inter_references_gt,
                ),
                dim=2,
            ) # (3ï¼Œ B, pad_size + all_query_num + 4*max_gt_num, 7) æ¯ä¸€å±‚Decoder layerçš„å¯¹åº”æ£€æµ‹ç»“æœ

        inter_references_out = inter_references
        '''
        ä»å‰å¾€åä¾æ¬¡è¿”å›: Decoder layeræ¯ä¸€å±‚çš„query, è¾“å…¥Decoderçš„å‚è€ƒæ¡†, Decoder layeræ¯ä¸€å±‚çš„æ£€æµ‹ç»“æœ, Encoderè¾“å‡ºçš„ç‰¹å¾å›¾, åˆå§‹åŒ–çš„å‚è€ƒæ¡†, egoçš„æœ€é«˜query_numçš„ç´¢å¼•
        TODO Encoderè¾“å‡ºçš„ç‰¹å¾å›¾ä¿¡æ¯ä¼šä¸ä¼šä¸è¶³? è¦ä¸è¦è€ƒè™‘å°†queryèåˆåçš„ä¿¡æ¯æ”¾å›å» ğŸŒŸUpdated: Done, å…ˆçœ‹çœ‹æ€§èƒ½
        '''
        return hs, init_reference_out, inter_references_out, memory, src_anchors, ego_topk_indexes

    def fuse_features_by_index(self, index_list, feature_list, fusion_func, extra_future, extra_index):
        """
        æ ¹æ®ç´¢å¼•å¯¹ç‰¹å¾è¿›è¡Œèåˆã€‚

        å‚æ•°:
        - index_list: list of torch.Tensor, å½¢çŠ¶ä¸º (1, n, 1) çš„ç´¢å¼•å¼ é‡åˆ—è¡¨ï¼Œæ¯ä¸ªè¡¨ç¤ºæœ‰æ•ˆçš„ç´¢å¼•ä½ç½®ã€‚ eg. [(1,300,1), (1,62,1)...]
        - feature_list: list of torch.Tensor, å½¢çŠ¶ä¸º (1, n, C) çš„ç‰¹å¾å›¾å¼ é‡åˆ—è¡¨ã€‚  eg. [(1,300,C), (1,62,C)...]
        - fusion_func: Callable, è‡ªå®šä¹‰èåˆå‡½æ•°, æ¥å—è¾“å…¥ (n, k, C)ï¼Œè¿”å›èåˆåçš„å¼ é‡ (n, 1, C),
                    å…¶ä¸­ k è¡¨ç¤ºå‚ä¸èåˆçš„ç‰¹å¾æ•°é‡ã€‚
        - extra_future: (1, 200, C), egoè‡ªèº«refineäº†500ä¸ªquery, å…¶ä¸­300ä¸ªå‚ä¸èåˆ, å200ä¸ªç”¨äºä»å‰åˆ°åå¡«å……ä¸é‡å çš„å…¶ä»–agentçš„query 
        - extra_index: (1, 200, 1)

        è¿”å›:
        - fused_features: torch.Tensor, èåˆåçš„ç‰¹å¾å¼ é‡, å½¢çŠ¶ä¸º (1, ego_query_num + extra_query_num, C)ã€‚  eg. (1, 300+200, C)
        """
        # æ£€æŸ¥è¾“å…¥åˆæ³•æ€§
        assert len(index_list) == len(feature_list), "ç´¢å¼•åˆ—è¡¨å’Œç‰¹å¾å›¾åˆ—è¡¨é•¿åº¦ä¸ä¸€è‡´"
        
        # ç»Ÿä¸€å¤„ç†ç´¢å¼•ï¼Œè·å–æ‰€æœ‰å”¯ä¸€ç´¢å¼•
        all_indices = torch.cat([idx.squeeze(0) for idx in index_list], dim=0)  # (sum(n), 1)
        # ç›¸åŒçš„ç´¢å¼•æ„å‘³ç€ç›¸åŒçš„ä½ç½®, (n_unique, ) å’Œé€†æ˜ å°„ (sum(n),) è¡¨ç¤ºæ¯ä¸ªå…ƒç´ åœ¨unique_indicesä¸­çš„ä½ç½®
        # FIXME ä»€ä¹ˆæƒ…å†µ? å³ä½¿è®¾ç½®ä¸ç”¨æ’åºï¼Œä½†æ˜¯æœ€åç»“æœä¾ç„¶æ’åºï¼Œæƒ³è¦ç¨³å®šå»é‡ï¼Œåªèƒ½è‡ªå·±å†™æ±‚unique
        # unique_indices, inverse_indices = torch.unique(all_indices, sorted=False, return_inverse=True) 

        seen = set()
        unique_vals = []
        for val in all_indices:
            scalar_val = val.item() # è¿™é‡Œdebugäº†å¥½ä¹…ï¼Œtensorå¯¹è±¡æ˜¯ä¸å¯å“ˆå¸Œçš„ï¼Œæ²¡ææ˜ç™½ç›´æ¥å¯¼è‡´è¿™é‡Œå»é‡å¤±è´¥ï¼Œè¿˜ä¼šå‡ºç°é‡å¤ï¼Œå› æ­¤å¿…é¡»è½¬ä¸ºpythonæ ‡é‡
            if scalar_val not in seen:
                seen.add(scalar_val)
                unique_vals.append(scalar_val)
        unique_indices = torch.tensor(unique_vals).to(all_indices)

        # æ„å»ºæ¯ä¸ªç´¢å¼•å¯¹åº”çš„ç‰¹å¾åˆ—è¡¨
        feature_map = {idx.item(): [] for idx in unique_indices} # eg. {id: [(1, C), ...]}
        for idx, features in zip(index_list, feature_list):
            for i, ind in enumerate(idx.squeeze(0).squeeze(-1)): # éå†æ¯ä¸ªagentçš„ç´¢å¼•
                feature_map[ind.item()].append(features[:, i, :])  # æŒ‰ç´¢å¼•å­˜å…¥ç‰¹å¾ (1, C)

        # å¯¹æ¯ä¸ªå”¯ä¸€ç´¢å¼•è¿›è¡Œèåˆ ç„¶åé‡æ–°æ”¾å›å» å½¢æˆ{unique_id: [feature]}
        fused_features = []  # å­˜å‚¨èåˆåçš„ç‰¹å¾
        for idx in unique_indices:
            features_to_fuse = torch.stack(feature_map[idx.item()], dim=1)  # (1, k, C) åŒä¸€ä¸ªç©ºé—´ä½ç½®æœ‰å¤šä¸ªfeature, å¯èƒ½æ˜¯egoå’Œå…¶ä»–agentï¼Œä¹Ÿå¯èƒ½æ˜¯agentä¹‹é—´
            fused_features.append(fusion_func(features_to_fuse)) # èåˆè¿”å›çš„åº”è¯¥æ˜¯(1, 1, C)
        fused_features = torch.cat(fused_features, dim=1)  # (1, n_unique, C)

        # ä» fused_features ä¸­æå–å±äº ego çš„ç‰¹å¾
        ego_indices = index_list[0].squeeze(0).squeeze(-1)  # ego çš„ç´¢å¼• ï¼ˆn1,ï¼‰ egoçš„ç´¢å¼•ä¸ªæ•°æ˜¯å›ºå®šçš„ï¼Œå°±ç­‰äºquery_num
        ego_mask = torch.isin(unique_indices, ego_indices)  # æ‰¾åˆ°å±äº ego çš„ç´¢å¼• (n_unique, ) egoå¯¹åº”çš„ç´¢å¼•å°±ä¸º True
        ego_features = fused_features[:, ego_mask, :]  # æå–å±äº ego çš„éƒ¨åˆ† (1, ego_query_size, C)

        non_overlap_features = []
        for idx, features in zip(index_list[1:], feature_list[1:]): # å¿½ç•¥ ego
            mask = ~torch.isin(idx.squeeze(0), index_list[0].squeeze(0)) # éé‡å éƒ¨åˆ† (n_unique, 1) XXX é¦–å…ˆå®Œå…¨é‡å ä¸å¯èƒ½ï¼Œé‚£åªæœ‰ä¸€ç§å¯èƒ½ï¼Œé‚£å°±æ˜¯agentå’Œegoæ„ŸçŸ¥èŒƒå›´éƒ½ä¸é‡åˆï¼Œæ‰€ä»¥æ ¹æœ¬å°±æ˜¯ç©º
            selected_features = features[:, mask.squeeze(), :] # æå–éé‡å ç‰¹å¾ (1, k', C)
            if selected_features.size(1) > 0:
                non_overlap_features.append(selected_features)

        # å°†éé‡å ç‰¹å¾æŒ‰åˆ†æ•°æˆªæ–­å¹¶å¡«å……åˆ°æœ€ç»ˆç»“æœä¸­
        if len(non_overlap_features) > 0:
            non_overlap_features = torch.cat(non_overlap_features, dim=1)  # (1, k_all, C)
            append_num = min(non_overlap_features.size(1), self.extra_query_num) # æœ€å¤§ä¸è¶…è¿‡ extra_query_num
            extra_future[:, :append_num, :] = non_overlap_features[:,:append_num,:]
        # else: # é¦–å…ˆèƒ½è¿›å…¥èåˆå‡½æ•°å°±è¯´æ˜æœ‰æŠ•å½±çš„queryå­˜åœ¨ï¼Œç»“æœéé‡å çš„ç‰¹å¾æ˜¯0ï¼Œè¿™å°±è¯´æ˜å…¨éƒ¨æ˜¯é‡å çš„ç‰¹å¾, ç»è¿‡éªŒè¯ï¼Œæ­¤æ—¶æŠ•å½±è¿‡æ¥çš„ç‰¹å¾æ•°é‡å¾ˆå°‘ï¼Œä¸€èˆ¬æ˜¯ä¸ªä½æ•°ï¼Œæå°‘æ•°æ—¶å€™æ˜¯å‡ å
        #     print("------------------------------------------------")
        #     print("Oops! All overlap???")
        #     print("unique_indices shape is ", unique_indices.shape)
        #     print("agent 1 shape is ", index_list[1].shape)
        #     print("------------------------------------------------")

        # æœ€ç»ˆç‰¹å¾: ego + extra_future
        final_features = torch.cat([ego_features, extra_future], dim=1)  # (1, ego_query_size + etra_query_num, C)

        unique_indices = unique_indices.unsqueeze(0).unsqueeze(-1) # (1, n_unique, 1)
        index_num = min(unique_indices.size(1), self.num_queries + self.extra_query_num)
        assert unique_indices.size(1) >= self.num_queries
        remain_start = index_num - self.num_queries
        final_indices = torch.cat([unique_indices[:, :index_num, :], extra_index[:, remain_start:, :]], dim = 1) # 500
        return final_features, final_indices

class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, nlevel, dim_feedforward, dropout, activation):
        super().__init__()
        self.self_attn = Box3dAttention(d_model, nlevel, nhead, with_rotation=False)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        self.activation = get_activation_fn(activation)

    @staticmethod
    def with_pos_embed(tensor, pos):
        return tensor if pos is None else tensor + pos

    def forward(self, src, pos, src_shape, src_start_idx, ref_windows):
        src2 = self.self_attn(
            self.with_pos_embed(src, pos),
            src,
            src_shape,
            None,
            src_start_idx,
            None,
            ref_windows,
        )

        src = src + self.dropout1(src2[0])
        src = self.norm1(src)

        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)

        return src


class TransformerEncoder(nn.Module):
    def __init__(self, d_model, encoder_layer, num_layers):
        super().__init__()
        self.layers = get_clones(encoder_layer, num_layers)

    def forward(self, src, pos, src_shape, src_start_idx, ref_windows):
        output = src
        for layer in self.layers:
            output = layer(output, pos, src_shape, src_start_idx, ref_windows)
        return output


class TransformerDecoderLayer(nn.Module):
    def __init__(self, d_model, nhead, nlevel, dim_feedforward, dropout, activation):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.multihead_attn = Box3dAttention(d_model, nlevel, nhead, with_rotation=True)

        self.pos_embed_layer = MLP(8, d_model, d_model, 3)

        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)
        self.dropout = nn.Dropout(dropout)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        self.activation = get_activation_fn(activation)

    @staticmethod
    def with_pos_embed(tensor, pos):
        return tensor if pos is None else tensor + pos

    def forward(self, idx, query, query_pos, memory, memory_shape, memory_start_idx, ref_windows, attn_mask=None):
        if idx == 0:
            query = self.pos_embed_layer(ref_windows)
            q = k = query
        elif query_pos is None:
            query_pos = self.pos_embed_layer(ref_windows)
            q = k = self.with_pos_embed(query, query_pos)

        q = q.transpose(0, 1)
        k = k.transpose(0, 1)
        v = query.transpose(0, 1)

        query2 = self.self_attn(q, k, v, attn_mask=attn_mask)[0]
        query2 = query2.transpose(0, 1)
        query = query + self.dropout1(query2)
        query = self.norm1(query)

        query2 = self.multihead_attn(
            self.with_pos_embed(query, query_pos),
            memory,
            memory_shape,
            None,
            memory_start_idx,
            None,
            ref_windows[..., :7],
        )[0]

        query = query + self.dropout2(query2)
        query = self.norm2(query)

        query2 = self.linear2(self.dropout(self.activation(self.linear1(query))))
        query = query + self.dropout3(query2)
        query = self.norm3(query)

        return query


class TransformerDecoder(nn.Module):
    def __init__(self, d_model, decoder_layer, num_layers, cp_flag):
        super().__init__()

        self.layers = get_clones(decoder_layer, num_layers)
        self.cp_flag = cp_flag # True
        if self.cp_flag:
            print("===ä½¿ç”¨checkpointä¼˜åŒ–å†…å­˜, ä½†æ˜¯ä¼šé™ä½è®­ç»ƒé€Ÿåº¦===")

    def forward(self, query, query_pos, memory, memory_shape, memory_start_idx, ref_windows, attn_mask=None):
        output = query
        intermediate = []
        intermediate_ref_windows = []
        for idx, layer in enumerate(self.layers):
            if self.cp_flag:
                output = cp.checkpoint(layer, idx, output, query_pos, memory, memory_shape, memory_start_idx, ref_windows, attn_mask)
            else:
                output = layer(idx, output, query_pos, memory, memory_shape, memory_start_idx, ref_windows, attn_mask)
            new_ref_logits, new_ref_windows = self.detection_head(output, ref_windows[..., :7], idx)
            new_ref_probs = new_ref_logits.sigmoid()
            ref_windows = torch.cat(
                (
                    new_ref_windows.detach(),
                    new_ref_probs.detach(),
                ),
                dim=-1,
            )
            intermediate.append(output)
            intermediate_ref_windows.append(new_ref_windows)
        return torch.stack(intermediate), torch.stack(intermediate_ref_windows)

class TransAdapt(nn.Module):
    def __init__(self, d_model, nhead, nlevel, dim_feedforward, dropout, activation):
        super().__init__()
        self.multihead_attn = Box3dAttention(d_model, nlevel, nhead, with_rotation=True)

        self.pos_embed_layer = MLP(8, d_model, d_model, 3)

        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        self.activation = get_activation_fn(activation)

    @staticmethod
    def with_pos_embed(tensor, pos):
        return tensor if pos is None else tensor + pos

    def forward(self, query, memory, memory_shape, memory_start_idx, ref_windows):
        '''
        query: (1, n, C)
        memory: (1, HW, C)
        memory_shape: (1,2)
        memory_start_idx: (0,)
        ref_windows: (1, n, 8)
        '''
        if query.size(1) == 0: # å¦‚æœå…¶ä»–agentçš„queryæ•°æ˜¯0ï¼Œé‚£å°±ç›´æ¥returnå³å¯
            return query
        query_pos = self.pos_embed_layer(ref_windows)

        query2 = self.multihead_attn(
            self.with_pos_embed(query, query_pos),
            memory,
            memory_shape,
            None,
            memory_start_idx,
            None,
            ref_windows[..., :7],
        )[0]

        query = query + self.dropout2(query2)
        query = self.norm2(query)

        query2 = self.linear2(self.dropout(self.activation(self.linear1(query))))
        query = query + self.dropout3(query2)
        query = self.norm3(query)

        return query


def get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])


def get_activation_fn(activation):
    """Return an activation function given a string"""
    if activation == "relu":
        return F.relu
    if activation == "gelu":
        return F.gelu
    if activation == "glu":
        return F.glu
    raise RuntimeError(f"activation should be relu/gelu, not {activation}.")


def flatten_with_shape(tensor_list, mask_list):
    """
    Params:
    :tensor_list: [(B, C, H1, W1), ..., (B, C, HN, WN)]
    :mask_list: [(B, H1, W1), ..., (B, HN, WN)]

    Return:
    :tensor_flatten: (B, L, C)
    :mask_flatten: (B, L)
    :tensor_shape: (N, 2)
    """
    assert isinstance(tensor_list, collections.abc.Sequence)
    assert len(tensor_list) > 0

    N = len(tensor_list) # 1
    tensor_shape = torch.zeros(N, 2, dtype=torch.int64, device=tensor_list[0].device) # (1, 2)
    tensor_flatten = []

    if mask_list is not None:
        mask_flatten = []

    for i, tensor in enumerate(tensor_list):
        new_tensor = tensor.flatten(2).permute(0, 2, 1) # å±•å¹³æˆï¼ˆBï¼ŒH*Wï¼ŒCï¼‰
        tensor_flatten.append(new_tensor)

        if mask_list is not None:
            mask = mask_list[i]
            new_mask = mask.flatten(1)
            mask_flatten.append(new_mask)
            assert tensor.shape[2] == mask.shape[1]
            assert tensor.shape[3] == mask.shape[2]
        tensor_shape[i, 0] = tensor.shape[2]
        tensor_shape[i, 1] = tensor.shape[3]

    mask_flatten = torch.cat(mask_flatten, dim=1) if mask_list is not None else None # (B, L)
    tensor_flatten = torch.cat(tensor_flatten, dim=1)

    return tensor_flatten, mask_flatten, tensor_shape